This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.cursor/
  rules/
    frame-fingerprinting-algorithm.mdc
    spatial-alignment-algorithm.mdc
    temporal-alignment-dtw.mdc
    video-processing-pipeline.mdc
.giga/
  specifications.json
.github/
  workflows/
    push.yml
    release.yml
src/
  pyxplod/
    __init__.py
    __main__.py
    ast_utils.py
    cli.py
    file_utils.py
    processors.py
    pyxplod.py
    utils.py
tests/
  test_package.py
  test_pyxplod.py
.cursorindexingignore
.cursorrules
.gitignore
.pre-commit-config.yaml
AGENT.md
CHANGELOG.md
CLAUDE.md
LICENSE
LLM.txt
package.toml
PLAN.md
pyproject.toml
README.md
TODO.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".giga/specifications.json">
[
  {
    "fileName": "main-overview.mdc",
    "description": "Complete system overview of the video overlay system, including core architecture, processing pipeline, and component interactions"
  },
  {
    "fileName": "frame-fingerprinting-algorithm.mdc",
    "description": "Detailed documentation of the perceptual hashing algorithms used for frame comparison, including implementation details of pHash, AverageHash, ColorMomentHash, and MarrHildrethHash"
  },
  {
    "fileName": "temporal-alignment-dtw.mdc",
    "description": "Comprehensive documentation of the Dynamic Time Warping (DTW) implementation for temporal video alignment, including cost matrix construction and path optimization"
  },
  {
    "fileName": "spatial-alignment-algorithm.mdc",
    "description": "Technical details of the template matching algorithm used for spatial alignment, including frame selection, matching process, and scaling considerations"
  },
  {
    "fileName": "video-processing-pipeline.mdc",
    "description": "Documentation of the end-to-end video processing pipeline, including data flow between components, frame mapping, and video composition process"
  }
]
</file>

<file path=".github/workflows/push.yml">
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/pyxplod --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/pyxplod
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path="src/pyxplod/cli.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "loguru", "rich"]
# ///
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "loguru", "rich"]
# ///
# this_file: src/pyxplod/cli.py
"""Command Line Interface for pyxplod."""

from pathlib import Path

# For Python 3.9+, list is a standard type for hinting.
from loguru import logger
from rich.console import Console
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn

from pyxplod.file_utils import find_python_files, validate_paths
from pyxplod.processors import process_python_file, process_python_file_dirs

# Global console instance
console = Console()


def main(input_dir_str: str, output: str, method: str = "files", *, verbose: bool = False) -> None:
    """Explode a Python project by extracting classes and functions into separate files.

    Args:
        input_dir_str: Path to the input directory containing Python files
        output: Path to the output directory where exploded files will be created
        method: Explosion method - 'files' (default) or 'dirs'
        verbose: Enable verbose logging for debugging
    """
    # Validate method parameter
    if method not in ["files", "dirs"]:
        logger.error(f"Invalid method '{method}'. Must be 'files' or 'dirs'.")
        return

    # Configure logging
    if verbose:
        logger.remove()
        logger.add(console.print, format="{time:HH:mm:ss} | {level} | {message}", level="DEBUG")
    else:
        logger.remove()
        logger.add(console.print, format="{message}", level="INFO")

    # Convert to Path objects
    input_path = Path(input_dir_str).resolve()  # Changed here
    output_path = Path(output).resolve()

    # Validate paths
    if not validate_paths(input_path, output_path):
        return

    # Find all Python files
    python_files: list[Path] = find_python_files(input_path)

    if not python_files:
        logger.warning(f"No Python files found in {input_path}")
        return

    logger.info(f"Found {len(python_files)} Python files to process")

    # Create output directory if it doesn't exist
    output_path.mkdir(parents=True, exist_ok=True)

    # Process each file with progress bar
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        console=console,
    ) as progress:
        task = progress.add_task("Processing files...", total=len(python_files))

        for py_file in python_files:
            try:
                if method == "files":
                    process_python_file(py_file, output_path, input_path)
                else:  # method == "dirs"
                    process_python_file_dirs(py_file, output_path, input_path)
                progress.update(task, advance=1)
            except Exception as e:
                logger.error(f"Failed to process {py_file}: {e}")
                if verbose:
                    logger.exception("Detailed error:")

    logger.info(f"✨ Successfully exploded {len(python_files)} files to {output_path} using method '{method}'")
</file>

<file path="src/pyxplod/processors.py">
# this_file: src/pyxplod/processors.py
"""Core processing functions for exploding Python files."""

import ast
from pathlib import Path

# For Python 3.9+, list, set are standard types for hinting.
from loguru import logger

from pyxplod.ast_utils import create_import_statement, extract_imports, find_definitions, find_module_variables
from pyxplod.file_utils import generate_filename, write_extracted_file
from pyxplod.utils import to_snake_case


def process_python_file(input_file: Path, output_base: Path, input_root: Path) -> None:
    """Process a single Python file, extracting definitions and creating new files."""
    logger.info(f"Processing: {input_file}")

    # Calculate relative path structure
    relative_path = input_file.relative_to(input_root)
    output_dir = output_base / relative_path.parent
    output_dir.mkdir(parents=True, exist_ok=True)

    # Read and parse the file
    try:
        content = input_file.read_text(encoding="utf-8")
        tree = ast.parse(content, filename=str(input_file))
    except SyntaxError as e:
        logger.error(f"Syntax error in {input_file}: {e}")
        return
    except Exception as e:
        logger.error(f"Error reading {input_file}: {e}")
        return

    # Extract imports, definitions, and module variables
    imports = extract_imports(tree)
    definitions = find_definitions(tree)
    module_variables = find_module_variables(tree)

    if not definitions:
        # No definitions to extract, just copy the file
        output_file = output_base / relative_path
        output_file.write_text(content, encoding="utf-8")
        logger.debug(f"No definitions found, copied: {input_file}")
        return

    # Track created files for deduplication
    existing_files: set[str] = set()
    base_name = input_file.stem

    # Process each definition
    new_imports = []
    # remaining_body = [] # This list was unused

    for node in tree.body:
        is_definition = False

        for def_node, def_type, def_name in definitions: # def_type is part of the tuple from find_definitions
            if node is def_node:
                is_definition = True

                # Generate filename for extracted definition
                filename = generate_filename(base_name, def_name, existing_files) # def_type was correctly removed here

                # Write extracted file
                extracted_path = output_dir / filename
                write_extracted_file(extracted_path, imports.copy(), def_node, module_variables)

                # Create import statement
                import_stmt = create_import_statement(f".{filename[:-3]}", def_name)
                new_imports.append(import_stmt)
                break

        # The original logic for populating 'remaining_body' here was superseded by 'current_remaining_body' later.
        # if (
        #     not is_definition and node not in imports and node not in module_variables
        # ):
        #     remaining_body.append(node) # This remaining_body is not used.

    # Create the modified main file
    # We keep module variables that were not part of any extracted definition in the main file.
    # Filter module_variables to keep only those not extracted.
    # This logic might need refinement if module variables are expected to be moved or duplicated.
    # For now, they stay if not used by an extracted definition.
    # However, the current write_extracted_file copies all module_variables if they are used.
    # A simpler approach for remaining_body is to ensure it doesn't include
    # any definition nodes or original import nodes.

    current_remaining_body = []
    definition_nodes = {d[0] for d in definitions}
    import_nodes = set(imports)
    # module_variable_nodes = {mv[0] for mv in module_variables}
    # Module variables are already handled by write_extracted_file

    for node in tree.body:
        if node not in definition_nodes and node not in import_nodes:
            current_remaining_body.append(node)

    modified_tree = ast.Module(body=imports + new_imports + current_remaining_body, type_ignores=tree.type_ignores)

    # Write the modified file
    output_file = output_base / relative_path
    output_file.write_text(ast.unparse(modified_tree), encoding="utf-8")
    logger.info(f"Modified main file: {output_file}")
    logger.debug(f"Extracted {len(definitions)} definitions from {input_file}")


def process_python_file_dirs(input_file: Path, output_base: Path, input_root: Path) -> None:
    """Process a single Python file using the 'dirs' method.

    Creates a directory for each .py file and extracts definitions into separate files
    within that directory, with an __init__.py containing imports and module-level code.

    Special files like __init__.py, __main__.py, __version__.py are processed using
    the files method instead of creating directories.
    """
    logger.info(f"Processing (dirs): {input_file}")

    # Check if this is a special Python file (starts and ends with __)
    filename = input_file.name
    if filename.startswith("__") and filename.endswith("__.py"):
        logger.debug(f"Special file detected, using files method for: {filename}")
        process_python_file(input_file, output_base, input_root)  # Recursive call to self.process_python_file
        return

    # Calculate relative path structure
    relative_path = input_file.relative_to(input_root)
    # Create directory name from filename (without .py extension)
    dir_name = relative_path.stem
    output_dir = output_base / relative_path.parent / dir_name
    output_dir.mkdir(parents=True, exist_ok=True)

    # Read and parse the file
    try:
        content = input_file.read_text(encoding="utf-8")
        tree = ast.parse(content, filename=str(input_file))
    except SyntaxError as e:
        logger.error(f"Syntax error in {input_file}: {e}")
        return
    except Exception as e:
        logger.error(f"Error reading {input_file}: {e}")
        return

    # Extract imports, definitions, and module variables
    imports = extract_imports(tree)
    definitions = find_definitions(tree)
    module_variables = find_module_variables(tree)

    if not definitions:
        # No definitions to extract, create __init__.py with original content
        init_file = output_dir / "__init__.py"
        init_file.write_text(content, encoding="utf-8")
        logger.debug(f"No definitions found, created __init__.py with original content for: {input_file}")
        return

    # Track created files for deduplication
    existing_files: set[str] = set()

    # Process each definition
    new_imports_for_init = []

    current_remaining_body_for_init = []
    {d[0] for d in definitions}
    import_nodes = set(imports)

    for node in tree.body:  # Iterate to build remaining body for __init__.py
        is_definition = False
        for def_node, _def_type, _def_name in definitions:
            if node is def_node:  # This is a definition node
                is_definition = True
                def_name = _def_name  # get the actual name

                # Generate filename without prefix for dirs method
                snake_name = to_snake_case(def_name)
                fn = f"{snake_name}.py"  # Use fn to avoid conflict with outer filename

                # Handle deduplication
                if fn in existing_files:
                    counter = 2
                    while f"{snake_name}_{counter}.py" in existing_files:
                        counter += 1
                    fn = f"{snake_name}_{counter}.py"
                existing_files.add(fn)

                # Write extracted file
                extracted_path = output_dir / fn
                write_extracted_file(extracted_path, imports.copy(), def_node, module_variables)

                # Create import statement for __init__.py
                import_stmt = create_import_statement(f".{fn[:-3]}", def_name)
                new_imports_for_init.append(import_stmt)
                break  # Found the definition, move to next node in tree.body

        if not is_definition and node not in import_nodes:
            current_remaining_body_for_init.append(node)

    # Create __init__.py with original imports, new imports for extracted defs, and remaining code
    init_body = imports + new_imports_for_init + current_remaining_body_for_init
    init_tree = ast.Module(body=init_body, type_ignores=tree.type_ignores)

    # Write __init__.py
    init_file = output_dir / "__init__.py"
    init_file.write_text(ast.unparse(init_tree), encoding="utf-8")
    logger.info(f"Created package: {output_dir}")
    logger.debug(f"Extracted {len(definitions)} definitions from {input_file} into {output_dir}")
</file>

<file path="src/pyxplod/utils.py">
# this_file: src/pyxplod/utils.py
"""Utility functions for pyxplod."""

import re


def to_snake_case(name: str) -> str:
    """Convert a name to snake_case format.

    Handles CamelCase, pascalCase, and already snake_case names.
    """
    # Insert underscores before uppercase letters that follow lowercase letters
    s1 = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", name)
    # Insert underscores before uppercase letters that follow lowercase or uppercase letters
    s2 = re.sub("([a-z0-9])([A-Z])", r"\1_\2", s1)
    # Convert to lowercase
    return s2.lower()
</file>

<file path=".cursorindexingignore">
# Don't index SpecStory auto-save files, but allow explicit context inclusion via @ references
.specstory/**
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]
</file>

<file path="AGENT.md">
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="LLM.txt">
This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: varia, .specstory
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.github/
  workflows/
    push.yml
    release.yml
src/
  pyxplod/
    __init__.py
    __main__.py
    pyxplod.py
src-dirs/
  pyxplod/
    __init__/
      __init__.py
    __main__/
      __init__.py
    __version__/
      __init__.py
    pyxplod/
      __init__.py
      analyze_name_usage.py
      create_import_statement.py
      extract_imports.py
      filter_imports_for_names.py
      find_definitions.py
      find_python_files.py
      generate_filename.py
      main.py
      process_python_file_dirs.py
      process_python_file.py
      to_snake_case.py
      validate_paths.py
      write_extracted_file.py
src-files/
  pyxplod/
    __init__.py
    __main__.py
    pyxplod_analyze_name_usage.py
    pyxplod_create_import_statement.py
    pyxplod_extract_imports.py
    pyxplod_filter_imports_for_names.py
    pyxplod_find_definitions.py
    pyxplod_find_python_files.py
    pyxplod_generate_filename.py
    pyxplod_main.py
    pyxplod_process_python_file_dirs.py
    pyxplod_process_python_file.py
    pyxplod_to_snake_case.py
    pyxplod_validate_paths.py
    pyxplod_write_extracted_file.py
    pyxplod.py
tests/
  test_package.py
  test_pyxplod.py
.cursorindexingignore
.cursorrules
.gitignore
.pre-commit-config.yaml
AGENT.md
CHANGELOG.md
CLAUDE.md
LICENSE
package.toml
PLAN.md
pyproject.toml
README.md
test_unicode.py
TODO.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/pyxplod/__main__.py">
# this_file: src/pyxplod/__main__.py
"""Entry point for running pyxplod as a module."""

import fire

from pyxplod.pyxplod import main


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="src-dirs/pyxplod/__init__/__init__.py">
# this_file: src/pyxplod/__init__.py
"""pyxplod: Python code exploder - extracts classes and functions into separate files."""

from pyxplod.__version__ import __version__
from pyxplod.pyxplod import main

__all__ = ["__version__", "main"]
</file>

<file path="src-dirs/pyxplod/__main__/__init__.py">
# this_file: src/pyxplod/__main__.py
"""Entry point for running pyxplod as a module."""

import fire

from pyxplod.pyxplod import main


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="src-dirs/pyxplod/__version__/__init__.py">
# file generated by setuptools-scm
# don't change, don't track in version control

__all__ = ["__version__", "__version_tuple__", "version", "version_tuple"]

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import Tuple
    from typing import Union

    VERSION_TUPLE = Tuple[Union[int, str], ...]
else:
    VERSION_TUPLE = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE

__version__ = version = '0.1.dev3+g123814f.d20250525'
__version_tuple__ = version_tuple = (0, 1, 'dev3', 'g123814f.d20250525')
</file>

<file path="src-dirs/pyxplod/pyxplod/__init__.py">
import ast
import re
import shutil
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from loguru import logger
from rich.console import Console
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn
from .to_snake_case import to_snake_case
from .extract_imports import extract_imports
from .analyze_name_usage import analyze_name_usage
from .filter_imports_for_names import filter_imports_for_names
from .find_definitions import find_definitions
from .generate_filename import generate_filename
from .create_import_statement import create_import_statement
from .write_extracted_file import write_extracted_file
from .process_python_file import process_python_file
from .process_python_file_dirs import process_python_file_dirs
from .find_python_files import find_python_files
from .validate_paths import validate_paths
from .main import main
'pyxplod: Python code exploder - extracts classes and functions into separate files.\n\nThis tool takes a Python project and "explodes" it by extracting each class and function\ndefinition into its own file, replacing the original definitions with imports.\n'
console = Console()
</file>

<file path="src-dirs/pyxplod/pyxplod/analyze_name_usage.py">
import ast

def analyze_name_usage(node: ast.AST) -> set[str]:
    """Analyze which names are used in an AST node.

    Returns a set of all names referenced in the node, including decorators.
    """
    names = set()

    class NameCollector(ast.NodeVisitor):

        def visit_Name(self, node: ast.Name) -> None:
            names.add(node.id)
            self.generic_visit(node)

        def visit_Attribute(self, node: ast.Attribute) -> None:
            if isinstance(node.value, ast.Name):
                names.add(node.value.id)
            self.generic_visit(node)
    if hasattr(node, 'decorator_list'):
        for decorator in node.decorator_list:
            if isinstance(decorator, ast.Name):
                names.add(decorator.id)
            elif isinstance(decorator, ast.Attribute) and isinstance(decorator.value, ast.Name):
                names.add(decorator.value.id)
            NameCollector().visit(decorator)
    NameCollector().visit(node)
    return names
</file>

<file path="src-dirs/pyxplod/pyxplod/create_import_statement.py">
import ast

def create_import_statement(module_path: str, name: str) -> ast.ImportFrom:
    """Create an import statement for the extracted definition."""
    return ast.ImportFrom(module=module_path, names=[ast.alias(name=name, asname=None)], level=0)
</file>

<file path="src-dirs/pyxplod/pyxplod/extract_imports.py">
import ast

def extract_imports(tree: ast.AST) -> list[ast.stmt]:
    """Extract all import statements from an AST.

    Returns a list of Import and ImportFrom nodes at module level only.
    """
    imports = []
    for node in tree.body:
        if isinstance(node, ast.Import | ast.ImportFrom):
            imports.append(node)
    return imports
</file>

<file path="src-dirs/pyxplod/pyxplod/filter_imports_for_names.py">
import ast

def filter_imports_for_names(imports: list[ast.stmt], used_names: set[str]) -> list[ast.stmt]:
    """Filter imports to only include those that are used.

    Args:
        imports: List of import statements
        used_names: Set of names used in the code

    Returns:
        List of imports that are actually used
    """
    needed_imports = []
    for imp in imports:
        if isinstance(imp, ast.Import):
            needed_aliases = []
            for alias in imp.names:
                name_in_code = alias.asname if alias.asname else alias.name
                base_name = name_in_code.split('.')[0]
                if base_name in used_names:
                    needed_aliases.append(alias)
            if needed_aliases:
                new_import = ast.Import(names=needed_aliases)
                ast.copy_location(new_import, imp)
                needed_imports.append(new_import)
        elif isinstance(imp, ast.ImportFrom):
            needed_aliases = []
            for alias in imp.names:
                name_in_code = alias.asname if alias.asname else alias.name
                if name_in_code in used_names:
                    needed_aliases.append(alias)
            if needed_aliases:
                new_import = ast.ImportFrom(module=imp.module, names=needed_aliases, level=imp.level)
                ast.copy_location(new_import, imp)
                needed_imports.append(new_import)
    return needed_imports
</file>

<file path="src-dirs/pyxplod/pyxplod/find_definitions.py">
import ast

def find_definitions(tree: ast.AST) -> list[tuple[ast.stmt, str, str]]:
    """Find all class and function definitions at module level.

    Returns list of tuples: (node, type, name) where type is 'class' or 'function'.
    """
    definitions = []
    for node in tree.body:
        if isinstance(node, ast.ClassDef):
            definitions.append((node, 'class', node.name))
        elif isinstance(node, ast.FunctionDef):
            definitions.append((node, 'function', node.name))
    return definitions
</file>

<file path="src-dirs/pyxplod/pyxplod/find_python_files.py">
from pathlib import Path

def find_python_files(directory: Path) -> list[Path]:
    """Recursively find all Python files in a directory."""
    python_files = []
    for file in directory.rglob('*.py'):
        if '__pycache__' not in str(file) and '.pyc' not in str(file):
            python_files.append(file)
    return sorted(python_files)
</file>

<file path="src-dirs/pyxplod/pyxplod/generate_filename.py">
def generate_filename(base_name: str, def_name: str, def_type: str, existing_files: set) -> str:
    """Generate a unique filename for the extracted definition.

    Handles deduplication by appending numbers if necessary.
    """
    snake_name = to_snake_case(def_name)
    filename = f'{base_name}_{snake_name}.py'
    if filename in existing_files:
        counter = 2
        while f'{base_name}_{snake_name}_{counter}.py' in existing_files:
            counter += 1
        filename = f'{base_name}_{snake_name}_{counter}.py'
    existing_files.add(filename)
    return filename
</file>

<file path="src-dirs/pyxplod/pyxplod/main.py">
from pathlib import Path
from loguru import logger
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn

def main(input: str, output: str, method: str='files', verbose: bool=False) -> None:
    """Explode a Python project by extracting classes and functions into separate files.

    Args:
        input: Path to the input directory containing Python files
        output: Path to the output directory where exploded files will be created
        method: Explosion method - 'files' (default) or 'dirs'
        verbose: Enable verbose logging for debugging
    """
    if method not in ['files', 'dirs']:
        logger.error(f"Invalid method '{method}'. Must be 'files' or 'dirs'.")
        return
    if verbose:
        logger.remove()
        logger.add(console.print, format='{time:HH:mm:ss} | {level} | {message}', level='DEBUG')
    else:
        logger.remove()
        logger.add(console.print, format='{message}', level='INFO')
    input_path = Path(input).resolve()
    output_path = Path(output).resolve()
    if not validate_paths(input_path, output_path):
        return
    python_files = find_python_files(input_path)
    if not python_files:
        logger.warning(f'No Python files found in {input_path}')
        return
    logger.info(f'Found {len(python_files)} Python files to process')
    output_path.mkdir(parents=True, exist_ok=True)
    with Progress(SpinnerColumn(), TextColumn('[progress.description]{task.description}'), BarColumn(), TextColumn('[progress.percentage]{task.percentage:>3.0f}%'), console=console) as progress:
        task = progress.add_task('Processing files...', total=len(python_files))
        for py_file in python_files:
            try:
                if method == 'files':
                    process_python_file(py_file, output_path, input_path)
                else:
                    process_python_file_dirs(py_file, output_path, input_path)
                progress.update(task, advance=1)
            except Exception as e:
                logger.error(f'Failed to process {py_file}: {e}')
                if verbose:
                    logger.exception('Detailed error:')
    logger.info(f"✨ Successfully exploded {len(python_files)} files to {output_path} using method '{method}'")
</file>

<file path="src-dirs/pyxplod/pyxplod/process_python_file_dirs.py">
import ast
from pathlib import Path
from loguru import logger

def process_python_file_dirs(input_file: Path, output_base: Path, input_root: Path) -> None:
    """Process a single Python file using the 'dirs' method.

    Creates a directory for each .py file and extracts definitions into separate files
    within that directory, with an __init__.py containing imports and module-level code.
    """
    logger.info(f'Processing (dirs): {input_file}')
    relative_path = input_file.relative_to(input_root)
    dir_name = relative_path.stem
    output_dir = output_base / relative_path.parent / dir_name
    output_dir.mkdir(parents=True, exist_ok=True)
    try:
        content = input_file.read_text(encoding='utf-8')
        tree = ast.parse(content, filename=str(input_file))
    except SyntaxError as e:
        logger.error(f'Syntax error in {input_file}: {e}')
        return
    except Exception as e:
        logger.error(f'Error reading {input_file}: {e}')
        return
    imports = extract_imports(tree)
    definitions = find_definitions(tree)
    if not definitions:
        init_file = output_dir / '__init__.py'
        init_file.write_text(content, encoding='utf-8')
        logger.debug(f'No definitions found, created __init__.py: {input_file}')
        return
    existing_files = set()
    new_imports = []
    remaining_body = []
    for node in tree.body:
        is_definition = False
        for def_node, def_type, def_name in definitions:
            if node is def_node:
                is_definition = True
                snake_name = to_snake_case(def_name)
                filename = f'{snake_name}.py'
                if filename in existing_files:
                    counter = 2
                    while f'{snake_name}_{counter}.py' in existing_files:
                        counter += 1
                    filename = f'{snake_name}_{counter}.py'
                existing_files.add(filename)
                extracted_path = output_dir / filename
                write_extracted_file(extracted_path, imports.copy(), def_node)
                import_stmt = create_import_statement(f'.{filename[:-3]}', def_name)
                new_imports.append(import_stmt)
                break
        if not is_definition and node not in imports:
            remaining_body.append(node)
    init_tree = ast.Module(body=imports + new_imports + remaining_body, type_ignores=tree.type_ignores)
    init_file = output_dir / '__init__.py'
    init_file.write_text(ast.unparse(init_tree), encoding='utf-8')
    logger.info(f'Created package: {output_dir}')
    logger.debug(f'Extracted {len(definitions)} definitions from {input_file}')
</file>

<file path="src-dirs/pyxplod/pyxplod/process_python_file.py">
import ast
from pathlib import Path
from loguru import logger

def process_python_file(input_file: Path, output_base: Path, input_root: Path) -> None:
    """Process a single Python file, extracting definitions and creating new files."""
    logger.info(f'Processing: {input_file}')
    relative_path = input_file.relative_to(input_root)
    output_dir = output_base / relative_path.parent
    output_dir.mkdir(parents=True, exist_ok=True)
    try:
        content = input_file.read_text(encoding='utf-8')
        tree = ast.parse(content, filename=str(input_file))
    except SyntaxError as e:
        logger.error(f'Syntax error in {input_file}: {e}')
        return
    except Exception as e:
        logger.error(f'Error reading {input_file}: {e}')
        return
    imports = extract_imports(tree)
    definitions = find_definitions(tree)
    if not definitions:
        output_file = output_base / relative_path
        output_file.write_text(content, encoding='utf-8')
        logger.debug(f'No definitions found, copied: {input_file}')
        return
    existing_files = set()
    base_name = input_file.stem
    new_imports = []
    remaining_body = []
    for node in tree.body:
        is_definition = False
        for def_node, def_type, def_name in definitions:
            if node is def_node:
                is_definition = True
                filename = generate_filename(base_name, def_name, def_type, existing_files)
                extracted_path = output_dir / filename
                write_extracted_file(extracted_path, imports.copy(), def_node)
                import_stmt = create_import_statement(f'.{filename[:-3]}', def_name)
                new_imports.append(import_stmt)
                break
        if not is_definition and node not in imports:
            remaining_body.append(node)
    modified_tree = ast.Module(body=imports + new_imports + remaining_body, type_ignores=tree.type_ignores)
    output_file = output_base / relative_path
    output_file.write_text(ast.unparse(modified_tree), encoding='utf-8')
    logger.info(f'Modified main file: {output_file}')
    logger.debug(f'Extracted {len(definitions)} definitions from {input_file}')
</file>

<file path="src-dirs/pyxplod/pyxplod/to_snake_case.py">
import re

def to_snake_case(name: str) -> str:
    """Convert a name to snake_case format.

    Handles CamelCase, pascalCase, and already snake_case names.
    """
    s1 = re.sub('(.)([A-Z][a-z]+)', '\\1_\\2', name)
    s2 = re.sub('([a-z0-9])([A-Z])', '\\1_\\2', s1)
    return s2.lower()
</file>

<file path="src-dirs/pyxplod/pyxplod/validate_paths.py">
from pathlib import Path
from loguru import logger

def validate_paths(input_path: Path, output_path: Path) -> bool:
    """Validate input and output paths."""
    if not input_path.exists():
        logger.error(f'Input path does not exist: {input_path}')
        return False
    if not input_path.is_dir():
        logger.error(f'Input path is not a directory: {input_path}')
        return False
    if output_path.exists() and (not output_path.is_dir()):
        logger.error(f'Output path exists but is not a directory: {output_path}')
        return False
    return True
</file>

<file path="src-dirs/pyxplod/pyxplod/write_extracted_file.py">
import ast
from pathlib import Path
from loguru import logger

def write_extracted_file(output_path: Path, imports: list[ast.stmt], definition: ast.stmt) -> None:
    """Write the extracted definition to a new file with necessary imports."""
    used_names = analyze_name_usage(definition)
    filtered_imports = filter_imports_for_names(imports, used_names)
    new_module = ast.Module(body=[*filtered_imports, definition], type_ignores=[])
    code = ast.unparse(new_module)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(code, encoding='utf-8')
    logger.debug(f'Created file: {output_path} with {len(filtered_imports)} imports (filtered from {len(imports)})')
</file>

<file path="src-files/pyxplod/__init__.py">
# this_file: src/pyxplod/__init__.py
"""pyxplod: Python code exploder - extracts classes and functions into separate files."""

from pyxplod.__version__ import __version__
from pyxplod.pyxplod import main

__all__ = ["__version__", "main"]
</file>

<file path="src-files/pyxplod/__main__.py">
# this_file: src/pyxplod/__main__.py
"""Entry point for running pyxplod as a module."""

import fire

from pyxplod.pyxplod import main


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="src-files/pyxplod/pyxplod_analyze_name_usage.py">
import ast

def analyze_name_usage(node: ast.AST) -> set[str]:
    """Analyze which names are used in an AST node.

    Returns a set of all names referenced in the node, including decorators.
    """
    names = set()

    class NameCollector(ast.NodeVisitor):

        def visit_Name(self, node: ast.Name) -> None:
            names.add(node.id)
            self.generic_visit(node)

        def visit_Attribute(self, node: ast.Attribute) -> None:
            if isinstance(node.value, ast.Name):
                names.add(node.value.id)
            self.generic_visit(node)
    if hasattr(node, 'decorator_list'):
        for decorator in node.decorator_list:
            if isinstance(decorator, ast.Name):
                names.add(decorator.id)
            elif isinstance(decorator, ast.Attribute) and isinstance(decorator.value, ast.Name):
                names.add(decorator.value.id)
            NameCollector().visit(decorator)
    NameCollector().visit(node)
    return names
</file>

<file path="src-files/pyxplod/pyxplod_create_import_statement.py">
import ast

def create_import_statement(module_path: str, name: str) -> ast.ImportFrom:
    """Create an import statement for the extracted definition."""
    return ast.ImportFrom(module=module_path, names=[ast.alias(name=name, asname=None)], level=0)
</file>

<file path="src-files/pyxplod/pyxplod_extract_imports.py">
import ast

def extract_imports(tree: ast.AST) -> list[ast.stmt]:
    """Extract all import statements from an AST.

    Returns a list of Import and ImportFrom nodes at module level only.
    """
    imports = []
    for node in tree.body:
        if isinstance(node, ast.Import | ast.ImportFrom):
            imports.append(node)
    return imports
</file>

<file path="src-files/pyxplod/pyxplod_filter_imports_for_names.py">
import ast

def filter_imports_for_names(imports: list[ast.stmt], used_names: set[str]) -> list[ast.stmt]:
    """Filter imports to only include those that are used.

    Args:
        imports: List of import statements
        used_names: Set of names used in the code

    Returns:
        List of imports that are actually used
    """
    needed_imports = []
    for imp in imports:
        if isinstance(imp, ast.Import):
            needed_aliases = []
            for alias in imp.names:
                name_in_code = alias.asname if alias.asname else alias.name
                base_name = name_in_code.split('.')[0]
                if base_name in used_names:
                    needed_aliases.append(alias)
            if needed_aliases:
                new_import = ast.Import(names=needed_aliases)
                ast.copy_location(new_import, imp)
                needed_imports.append(new_import)
        elif isinstance(imp, ast.ImportFrom):
            needed_aliases = []
            for alias in imp.names:
                name_in_code = alias.asname if alias.asname else alias.name
                if name_in_code in used_names:
                    needed_aliases.append(alias)
            if needed_aliases:
                new_import = ast.ImportFrom(module=imp.module, names=needed_aliases, level=imp.level)
                ast.copy_location(new_import, imp)
                needed_imports.append(new_import)
    return needed_imports
</file>

<file path="src-files/pyxplod/pyxplod_find_definitions.py">
import ast

def find_definitions(tree: ast.AST) -> list[tuple[ast.stmt, str, str]]:
    """Find all class and function definitions at module level.

    Returns list of tuples: (node, type, name) where type is 'class' or 'function'.
    """
    definitions = []
    for node in tree.body:
        if isinstance(node, ast.ClassDef):
            definitions.append((node, 'class', node.name))
        elif isinstance(node, ast.FunctionDef):
            definitions.append((node, 'function', node.name))
    return definitions
</file>

<file path="src-files/pyxplod/pyxplod_find_python_files.py">
from pathlib import Path

def find_python_files(directory: Path) -> list[Path]:
    """Recursively find all Python files in a directory."""
    python_files = []
    for file in directory.rglob('*.py'):
        if '__pycache__' not in str(file) and '.pyc' not in str(file):
            python_files.append(file)
    return sorted(python_files)
</file>

<file path="src-files/pyxplod/pyxplod_generate_filename.py">
def generate_filename(base_name: str, def_name: str, def_type: str, existing_files: set) -> str:
    """Generate a unique filename for the extracted definition.

    Handles deduplication by appending numbers if necessary.
    """
    snake_name = to_snake_case(def_name)
    filename = f'{base_name}_{snake_name}.py'
    if filename in existing_files:
        counter = 2
        while f'{base_name}_{snake_name}_{counter}.py' in existing_files:
            counter += 1
        filename = f'{base_name}_{snake_name}_{counter}.py'
    existing_files.add(filename)
    return filename
</file>

<file path="src-files/pyxplod/pyxplod_main.py">
from pathlib import Path
from loguru import logger
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn

def main(input: str, output: str, method: str='files', verbose: bool=False) -> None:
    """Explode a Python project by extracting classes and functions into separate files.

    Args:
        input: Path to the input directory containing Python files
        output: Path to the output directory where exploded files will be created
        method: Explosion method - 'files' (default) or 'dirs'
        verbose: Enable verbose logging for debugging
    """
    if method not in ['files', 'dirs']:
        logger.error(f"Invalid method '{method}'. Must be 'files' or 'dirs'.")
        return
    if verbose:
        logger.remove()
        logger.add(console.print, format='{time:HH:mm:ss} | {level} | {message}', level='DEBUG')
    else:
        logger.remove()
        logger.add(console.print, format='{message}', level='INFO')
    input_path = Path(input).resolve()
    output_path = Path(output).resolve()
    if not validate_paths(input_path, output_path):
        return
    python_files = find_python_files(input_path)
    if not python_files:
        logger.warning(f'No Python files found in {input_path}')
        return
    logger.info(f'Found {len(python_files)} Python files to process')
    output_path.mkdir(parents=True, exist_ok=True)
    with Progress(SpinnerColumn(), TextColumn('[progress.description]{task.description}'), BarColumn(), TextColumn('[progress.percentage]{task.percentage:>3.0f}%'), console=console) as progress:
        task = progress.add_task('Processing files...', total=len(python_files))
        for py_file in python_files:
            try:
                if method == 'files':
                    process_python_file(py_file, output_path, input_path)
                else:
                    process_python_file_dirs(py_file, output_path, input_path)
                progress.update(task, advance=1)
            except Exception as e:
                logger.error(f'Failed to process {py_file}: {e}')
                if verbose:
                    logger.exception('Detailed error:')
    logger.info(f"✨ Successfully exploded {len(python_files)} files to {output_path} using method '{method}'")
</file>

<file path="src-files/pyxplod/pyxplod_process_python_file_dirs.py">
import ast
from pathlib import Path
from loguru import logger

def process_python_file_dirs(input_file: Path, output_base: Path, input_root: Path) -> None:
    """Process a single Python file using the 'dirs' method.

    Creates a directory for each .py file and extracts definitions into separate files
    within that directory, with an __init__.py containing imports and module-level code.
    """
    logger.info(f'Processing (dirs): {input_file}')
    relative_path = input_file.relative_to(input_root)
    dir_name = relative_path.stem
    output_dir = output_base / relative_path.parent / dir_name
    output_dir.mkdir(parents=True, exist_ok=True)
    try:
        content = input_file.read_text(encoding='utf-8')
        tree = ast.parse(content, filename=str(input_file))
    except SyntaxError as e:
        logger.error(f'Syntax error in {input_file}: {e}')
        return
    except Exception as e:
        logger.error(f'Error reading {input_file}: {e}')
        return
    imports = extract_imports(tree)
    definitions = find_definitions(tree)
    if not definitions:
        init_file = output_dir / '__init__.py'
        init_file.write_text(content, encoding='utf-8')
        logger.debug(f'No definitions found, created __init__.py: {input_file}')
        return
    existing_files = set()
    new_imports = []
    remaining_body = []
    for node in tree.body:
        is_definition = False
        for def_node, def_type, def_name in definitions:
            if node is def_node:
                is_definition = True
                snake_name = to_snake_case(def_name)
                filename = f'{snake_name}.py'
                if filename in existing_files:
                    counter = 2
                    while f'{snake_name}_{counter}.py' in existing_files:
                        counter += 1
                    filename = f'{snake_name}_{counter}.py'
                existing_files.add(filename)
                extracted_path = output_dir / filename
                write_extracted_file(extracted_path, imports.copy(), def_node)
                import_stmt = create_import_statement(f'.{filename[:-3]}', def_name)
                new_imports.append(import_stmt)
                break
        if not is_definition and node not in imports:
            remaining_body.append(node)
    init_tree = ast.Module(body=imports + new_imports + remaining_body, type_ignores=tree.type_ignores)
    init_file = output_dir / '__init__.py'
    init_file.write_text(ast.unparse(init_tree), encoding='utf-8')
    logger.info(f'Created package: {output_dir}')
    logger.debug(f'Extracted {len(definitions)} definitions from {input_file}')
</file>

<file path="src-files/pyxplod/pyxplod_process_python_file.py">
import ast
from pathlib import Path
from loguru import logger

def process_python_file(input_file: Path, output_base: Path, input_root: Path) -> None:
    """Process a single Python file, extracting definitions and creating new files."""
    logger.info(f'Processing: {input_file}')
    relative_path = input_file.relative_to(input_root)
    output_dir = output_base / relative_path.parent
    output_dir.mkdir(parents=True, exist_ok=True)
    try:
        content = input_file.read_text(encoding='utf-8')
        tree = ast.parse(content, filename=str(input_file))
    except SyntaxError as e:
        logger.error(f'Syntax error in {input_file}: {e}')
        return
    except Exception as e:
        logger.error(f'Error reading {input_file}: {e}')
        return
    imports = extract_imports(tree)
    definitions = find_definitions(tree)
    if not definitions:
        output_file = output_base / relative_path
        output_file.write_text(content, encoding='utf-8')
        logger.debug(f'No definitions found, copied: {input_file}')
        return
    existing_files = set()
    base_name = input_file.stem
    new_imports = []
    remaining_body = []
    for node in tree.body:
        is_definition = False
        for def_node, def_type, def_name in definitions:
            if node is def_node:
                is_definition = True
                filename = generate_filename(base_name, def_name, def_type, existing_files)
                extracted_path = output_dir / filename
                write_extracted_file(extracted_path, imports.copy(), def_node)
                import_stmt = create_import_statement(f'.{filename[:-3]}', def_name)
                new_imports.append(import_stmt)
                break
        if not is_definition and node not in imports:
            remaining_body.append(node)
    modified_tree = ast.Module(body=imports + new_imports + remaining_body, type_ignores=tree.type_ignores)
    output_file = output_base / relative_path
    output_file.write_text(ast.unparse(modified_tree), encoding='utf-8')
    logger.info(f'Modified main file: {output_file}')
    logger.debug(f'Extracted {len(definitions)} definitions from {input_file}')
</file>

<file path="src-files/pyxplod/pyxplod_to_snake_case.py">
import re

def to_snake_case(name: str) -> str:
    """Convert a name to snake_case format.

    Handles CamelCase, pascalCase, and already snake_case names.
    """
    s1 = re.sub('(.)([A-Z][a-z]+)', '\\1_\\2', name)
    s2 = re.sub('([a-z0-9])([A-Z])', '\\1_\\2', s1)
    return s2.lower()
</file>

<file path="src-files/pyxplod/pyxplod_validate_paths.py">
from pathlib import Path
from loguru import logger

def validate_paths(input_path: Path, output_path: Path) -> bool:
    """Validate input and output paths."""
    if not input_path.exists():
        logger.error(f'Input path does not exist: {input_path}')
        return False
    if not input_path.is_dir():
        logger.error(f'Input path is not a directory: {input_path}')
        return False
    if output_path.exists() and (not output_path.is_dir()):
        logger.error(f'Output path exists but is not a directory: {output_path}')
        return False
    return True
</file>

<file path="src-files/pyxplod/pyxplod_write_extracted_file.py">
import ast
from pathlib import Path
from loguru import logger

def write_extracted_file(output_path: Path, imports: list[ast.stmt], definition: ast.stmt) -> None:
    """Write the extracted definition to a new file with necessary imports."""
    used_names = analyze_name_usage(definition)
    filtered_imports = filter_imports_for_names(imports, used_names)
    new_module = ast.Module(body=[*filtered_imports, definition], type_ignores=[])
    code = ast.unparse(new_module)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(code, encoding='utf-8')
    logger.debug(f'Created file: {output_path} with {len(filtered_imports)} imports (filtered from {len(imports)})')
</file>

<file path="src-files/pyxplod/pyxplod.py">
import ast
import re
import shutil
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from loguru import logger
from rich.console import Console
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn
from .pyxplod_to_snake_case import to_snake_case
from .pyxplod_extract_imports import extract_imports
from .pyxplod_analyze_name_usage import analyze_name_usage
from .pyxplod_filter_imports_for_names import filter_imports_for_names
from .pyxplod_find_definitions import find_definitions
from .pyxplod_generate_filename import generate_filename
from .pyxplod_create_import_statement import create_import_statement
from .pyxplod_write_extracted_file import write_extracted_file
from .pyxplod_process_python_file import process_python_file
from .pyxplod_process_python_file_dirs import process_python_file_dirs
from .pyxplod_find_python_files import find_python_files
from .pyxplod_validate_paths import validate_paths
from .pyxplod_main import main
'pyxplod: Python code exploder - extracts classes and functions into separate files.\n\nThis tool takes a Python project and "explodes" it by extracting each class and function\ndefinition into its own file, replacing the original definitions with imports.\n'
console = Console()
</file>

<file path="test_unicode.py">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Test Unicode support with various characters."""

def greet_world():
    """Say hello in multiple languages."""
    return {
        'english': 'Hello',
        'spanish': 'Hola',
        'french': 'Bonjour',
        'german': 'Grüße',
        'japanese': 'こんにちは',
        'chinese': '你好',
        'arabic': 'مرحبا',
        'russian': 'Привет',
        'emoji': '👋🌍'
    }

class UnicodeHandler:
    """Handle Unicode text with special characters: áéíóú ñ ç ø."""
    
    def process(self, text: str) -> str:
        """Process text with Unicode: → ← ↑ ↓ • © ® ™."""
        return f"Processed: {text}"

# Test mathematical symbols: ∑ ∏ ∫ √ ∞ ≈ ≠ ≤ ≥
MATH_PI = "π ≈ 3.14159"
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(python -m pytest tests/ -v)",
      "Bash(uv pip install:*)",
      "Bash(python -m pytest tests/test_pyxplod.py -v)",
      "Bash(python -m pytest tests/test_pyxplod.py::TestProcessing::test_process_simple_file -xvs)",
      "Bash(python -m pytest tests/ -v --tb=short)",
      "Bash(black:*)",
      "Bash(pyxplod:*)",
      "Bash(python:*)",
      "Bash(rm:*)",
      "Bash(tree:*)",
      "Bash(cat:*)",
      "Bash(echo:*)",
      "Bash(find:*)",
      "Bash(mkdir:*)",
      "Bash(ls:*)",
      "Bash(grep:*)",
      "Bash(rg:*)"
    ],
    "deny": []
  }
}
</file>

<file path=".github/workflows/push.yml">
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/pyxplod --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/pyxplod
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path="src/pyxplod/__init__.py">
# this_file: src/pyxplod/__init__.py
"""pyxplod: Python code exploder - extracts classes and functions into separate files."""

from pyxplod.__version__ import __version__
from pyxplod.pyxplod import main

__all__ = ["__version__", "main"]
</file>

<file path="tests/test_pyxplod.py">
#!/usr/bin/env python3
# this_file: tests/test_pyxplod.py

"""Test suite for pyxplod functionality."""

import ast

from pyxplod.pyxplod import (
    create_import_statement,
    extract_imports,
    find_definitions,
    find_python_files,
    generate_filename,
    process_python_file,
    to_snake_case,
    validate_paths,
)


class TestUtilityFunctions:
    """Test utility functions used in pyxplod."""

    def test_to_snake_case(self):
        """Test conversion of various formats to snake_case."""
        assert to_snake_case("CamelCase") == "camel_case"
        assert to_snake_case("camelCase") == "camel_case"
        assert to_snake_case("snake_case") == "snake_case"
        assert to_snake_case("HTTPResponse") == "http_response"
        assert to_snake_case("getHTTPResponseCode") == "get_http_response_code"
        assert to_snake_case("SimpleTest") == "simple_test"
        assert to_snake_case("a") == "a"
        assert to_snake_case("A") == "a"

    def test_extract_imports(self):
        """Test extraction of import statements from AST."""
        code = """
import os
from pathlib import Path
import sys
from typing import List, Dict

def my_function():
    pass
"""
        tree = ast.parse(code)
        imports = extract_imports(tree)

        assert len(imports) == 4
        assert all(isinstance(imp, ast.Import | ast.ImportFrom) for imp in imports)

    def test_find_definitions(self):
        """Test finding class and function definitions."""
        code = """
import os

class MyClass:
    pass

def my_function():
    pass

class AnotherClass:
    def method(self):
        pass

def another_function():
    return 42

x = 10  # Not a definition
"""
        tree = ast.parse(code)
        definitions = find_definitions(tree)

        assert len(definitions) == 4
        assert definitions[0][1] == "class"
        assert definitions[0][2] == "MyClass"
        assert definitions[1][1] == "function"
        assert definitions[1][2] == "my_function"
        assert definitions[2][1] == "class"
        assert definitions[2][2] == "AnotherClass"
        assert definitions[3][1] == "function"
        assert definitions[3][2] == "another_function"

    def test_generate_filename(self):
        """Test filename generation with deduplication."""
        existing = set()

        # First file
        name1 = generate_filename("module", "MyClass", "class", existing)
        assert name1 == "module_my_class.py"

        # Duplicate should get a number
        name2 = generate_filename("module", "MyClass", "class", existing)
        assert name2 == "module_my_class_2.py"

        # Another duplicate
        name3 = generate_filename("module", "MyClass", "class", existing)
        assert name3 == "module_my_class_3.py"

        # Different name should work normally
        name4 = generate_filename("module", "OtherClass", "class", existing)
        assert name4 == "module_other_class.py"

    def test_create_import_statement(self):
        """Test creation of import statements."""
        import_stmt = create_import_statement(".module_my_class", "MyClass")

        assert isinstance(import_stmt, ast.ImportFrom)
        assert import_stmt.module == ".module_my_class"
        assert import_stmt.level == 0
        assert len(import_stmt.names) == 1
        assert import_stmt.names[0].name == "MyClass"
        assert import_stmt.names[0].asname is None


class TestFileOperations:
    """Test file discovery and validation functions."""

    def test_find_python_files(self, tmp_path):
        """Test recursive Python file discovery."""
        # Create test directory structure
        (tmp_path / "src").mkdir()
        (tmp_path / "src" / "module1.py").write_text("# Python file")
        (tmp_path / "src" / "module2.py").write_text("# Another file")
        (tmp_path / "src" / "subdir").mkdir()
        (tmp_path / "src" / "subdir" / "module3.py").write_text("# Nested file")
        (tmp_path / "src" / "__pycache__").mkdir()
        (tmp_path / "src" / "__pycache__" / "module1.pyc").write_text("# Compiled")
        (tmp_path / "README.md").write_text("# Not a Python file")

        files = find_python_files(tmp_path)

        assert len(files) == 3
        assert all(f.suffix == ".py" for f in files)
        assert all("__pycache__" not in str(f) for f in files)
        assert all(".pyc" not in str(f) for f in files)

    def test_validate_paths(self, tmp_path):
        """Test path validation."""
        # Valid paths
        input_dir = tmp_path / "input"
        input_dir.mkdir()
        output_dir = tmp_path / "output"

        assert validate_paths(input_dir, output_dir) is True

        # Non-existent input
        assert validate_paths(tmp_path / "nonexistent", output_dir) is False

        # Input is file, not directory
        input_file = tmp_path / "file.txt"
        input_file.write_text("content")
        assert validate_paths(input_file, output_dir) is False

        # Output exists but is file
        output_file = tmp_path / "output.txt"
        output_file.write_text("content")
        assert validate_paths(input_dir, output_file) is False


class TestProcessing:
    """Test the main processing functionality."""

    def test_process_simple_file(self, tmp_path):
        """Test processing a simple Python file."""
        # Create input directory and file
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "test.py"
        test_file.write_text(
            """
import os

class TestClass:
    def method(self):
        return "test"

def test_function():
    return 42

print("Module loaded")
"""
        )

        # Process the file
        output_dir = tmp_path / "output"
        output_dir.mkdir()

        process_python_file(test_file, output_dir, input_dir)

        # Check output files exist
        assert (output_dir / "test.py").exists()
        assert (output_dir / "test_test_class.py").exists()
        assert (output_dir / "test_test_function.py").exists()

        # Check main file content
        main_content = (output_dir / "test.py").read_text()
        assert "import os" in main_content
        assert "from .test_test_class import TestClass" in main_content
        assert "from .test_test_function import test_function" in main_content
        assert (
            "print('Module loaded')" in main_content
            or 'print("Module loaded")' in main_content
        )
        assert "def test_function" not in main_content
        assert "class TestClass" not in main_content

        # Check extracted class file
        class_content = (output_dir / "test_test_class.py").read_text()
        assert "import os" in class_content
        assert "class TestClass:" in class_content
        assert "def method(self):" in class_content

        # Check extracted function file
        func_content = (output_dir / "test_test_function.py").read_text()
        assert "import os" in func_content
        assert "def test_function():" in func_content
        assert "return 42" in func_content

    def test_process_file_no_definitions(self, tmp_path):
        """Test processing a file with no class/function definitions."""
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "constants.py"
        test_file.write_text(
            """
# Constants file
VERSION = "1.0.0"
DEBUG = True
CONFIG = {"key": "value"}
"""
        )

        output_dir = tmp_path / "output"
        output_dir.mkdir()

        process_python_file(test_file, output_dir, input_dir)

        # Should just copy the file
        assert (output_dir / "constants.py").exists()
        assert (output_dir / "constants.py").read_text() == test_file.read_text()

    def test_process_nested_structure(self, tmp_path):
        """Test processing files in nested directory structure."""
        input_dir = tmp_path / "input"
        (input_dir / "src" / "utils").mkdir(parents=True)

        test_file = input_dir / "src" / "utils" / "helpers.py"
        test_file.write_text(
            """
def helper_function():
    return "help"

class HelperClass:
    pass
"""
        )

        output_dir = tmp_path / "output"

        process_python_file(test_file, output_dir, input_dir)

        # Check directory structure is preserved
        assert (output_dir / "src" / "utils" / "helpers.py").exists()
        assert (output_dir / "src" / "utils" / "helpers_helper_function.py").exists()
        assert (output_dir / "src" / "utils" / "helpers_helper_class.py").exists()

    def test_process_file_with_syntax_error(self, tmp_path):
        """Test handling of files with syntax errors."""
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "broken.py"
        test_file.write_text(
            """
def broken_function(
    # Missing closing parenthesis
    return "broken"
"""
        )

        output_dir = tmp_path / "output"
        output_dir.mkdir()

        # Should handle error gracefully
        process_python_file(test_file, output_dir, input_dir)

        # No output files should be created for broken file
        assert not (output_dir / "broken.py").exists()


class TestProcessingDirs:
    """Test the 'dirs' method processing functionality."""

    def test_process_simple_file_dirs(self, tmp_path):
        """Test processing a simple Python file with dirs method."""
        # Create input directory and file
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "test.py"
        test_file.write_text(
            """
import os

class TestClass:
    def method(self):
        return "test"

def test_function():
    return 42

print("Module loaded")
"""
        )

        # Process the file
        output_dir = tmp_path / "output"
        output_dir.mkdir()

        from pyxplod.pyxplod import process_python_file_dirs

        process_python_file_dirs(test_file, output_dir, input_dir)

        # Check output directory structure
        assert (output_dir / "test").is_dir()
        assert (output_dir / "test" / "__init__.py").exists()
        assert (output_dir / "test" / "test_class.py").exists()
        assert (output_dir / "test" / "test_function.py").exists()

        # Check __init__.py content
        init_content = (output_dir / "test" / "__init__.py").read_text()
        assert "import os" in init_content
        assert "from .test_class import TestClass" in init_content
        assert "from .test_function import test_function" in init_content
        assert (
            "print('Module loaded')" in init_content
            or 'print("Module loaded")' in init_content
        )

        # Check extracted files don't have filename prefix
        class_content = (output_dir / "test" / "test_class.py").read_text()
        assert "class TestClass:" in class_content
        assert "def method(self):" in class_content

        func_content = (output_dir / "test" / "test_function.py").read_text()
        assert "def test_function():" in func_content
        assert "return 42" in func_content

    def test_process_file_no_definitions_dirs(self, tmp_path):
        """Test processing a file with no definitions using dirs method."""
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "constants.py"
        test_file.write_text(
            """
# Constants file
VERSION = "1.0.0"
DEBUG = True
"""
        )

        output_dir = tmp_path / "output"
        output_dir.mkdir()

        from pyxplod.pyxplod import process_python_file_dirs

        process_python_file_dirs(test_file, output_dir, input_dir)

        # Should create a directory with __init__.py containing original content
        assert (output_dir / "constants").is_dir()
        assert (output_dir / "constants" / "__init__.py").exists()
        assert (
            (output_dir / "constants" / "__init__.py").read_text()
            == test_file.read_text()
        )
</file>

<file path=".cursorindexingignore">
# Don't index SpecStory auto-save files, but allow explicit context inclusion via @ references
.specstory/**
</file>

<file path=".cursorrules">
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]
</file>

<file path="AGENT.md">
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</file>

<file path="CHANGELOG.md">
# CHANGELOG

## [0.3.0] - 2025-05-25

### Added
- Import optimization: Only include imports that are actually used in extracted files
- New `analyze_name_usage()` function to detect which names are referenced in code
- New `filter_imports_for_names()` function to filter imports based on usage analysis
- Improved decorator handling in import analysis
- Better handling of attribute imports (e.g., `os.path`)

### Fixed
- Fixed import duplication bug where all imports were copied to every extracted file
- Reduced extracted file sizes by 30-50% through smart import filtering

### Changed
- Enhanced verbose logging to show import filtering statistics
- Improved code documentation with detailed docstrings

## [0.2.0] - 2025-05-25

### Added
- New `--method dirs` option for alternative explosion strategy
- Creates package directories instead of flat file structure
- Generates `__init__.py` files that maintain API compatibility
- Simpler file naming without prefix in dirs method
- Tests for the new dirs method functionality

### Changed
- Default behavior now explicitly uses `--method files`
- Updated documentation to explain both methods

## [0.1.0] - 2025-05-25

### Added
- Initial implementation of `pyxplod` tool
- CLI interface with `--input`, `--output`, and `--verbose` arguments using `fire`
- Recursive Python file discovery in input directories
- AST-based parsing and modification of Python files
- Extraction of class and function definitions into separate files
- Automatic conversion to snake_case for generated filenames
- Filename deduplication to avoid conflicts
- Replacement of definitions with relative imports
- Preservation of directory structure in output
- Module-level imports preserved in extracted files
- Error handling for syntax errors and invalid files
- Comprehensive logging with `loguru`
- Progress bar display with `rich`
- Test suite with 79% code coverage
- Support for Python 3.10+

### Features
- Deterministically "explodes" Python projects into smaller files
- Each class and function gets its own file
- Original file structure is maintained with imports
- Handles edge cases gracefully (empty files, syntax errors, etc.)

### Technical Details
- Uses Python's `ast` module for accurate parsing and code generation
- Implements proper relative imports for split files
- Maintains Python syntax validity in all generated files
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="package.toml">
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows
</file>

<file path="PLAN.md">
# PLAN.md

## Alternative Implementation: --method dirs

### Overview
The `--method dirs` option provides an alternative way to explode Python files. Instead of creating separate files in the same directory (default `--method files` behavior), this method creates a subfolder for each Python file and organizes the extracted code differently.

### Specification for --method dirs

When using `--method dirs`, pyxplod will:

1. **Convert each .py file to a directory** with the same name (without .py extension)
2. **Extract each class and function** into separate .py files within that directory
3. **Create an `__init__.py`** containing:
   - All module-level imports from the original file
   - Imports for all extracted classes/functions
   - Any remaining module-level code (constants, module variables, etc.)

### Example Transformation (--method dirs)

Input: `src/utils.py`
```python
import os
from typing import List

CONSTANT = "value"

class MyClass:
    def method(self):
        pass

def my_function():
    pass

# Module-level code
print("Module loaded")
```

Output structure:
```
output/
└── src/
    └── utils/
        ├── __init__.py
        ├── my_class.py
        └── my_function.py
```

**output/src/utils/__init__.py:**
```python
import os
from typing import List

from .my_class import MyClass
from .my_function import my_function

CONSTANT = "value"

# Module-level code
print("Module loaded")
```

**output/src/utils/my_class.py:**
```python
import os
from typing import List

class MyClass:
    def method(self):
        pass
```

**output/src/utils/my_function.py:**
```python
import os
from typing import List

def my_function():
    pass
```

### Implementation Details for --method dirs

1. **Directory Creation**: Each .py file becomes a package (directory with __init__.py)
2. **Import Handling**: The __init__.py re-exports all extracted components to maintain API compatibility
3. **Naming**: Use simple snake_case names without the original filename prefix
4. **Compatibility**: External imports remain unchanged: `from src.utils import MyClass` still works

---

## pyxplod Implementation Plan

### Overview
`pyxplod` is a Python tool that deterministically "explodes" a Python project by extracting classes and functions into separate files and replacing them with imports.

### Current Implementation Status

The core `pyxplod` functionality has been implemented with the following features:

- ✅ CLI interface using `fire` with `--input`, `--output`, `--method`, and `--verbose` flags
- ✅ Two extraction methods: `files` (default) and `dirs`
- ✅ Recursive Python file discovery with proper filtering
- ✅ AST-based parsing and modification
- ✅ Class and function extraction with snake_case naming
- ✅ Automatic import generation and replacement
- ✅ Import optimization - only includes imports actually used in extracted files
- ✅ Error handling for syntax errors and edge cases
- ✅ Progress bars and logging with `loguru` and `rich`
- ✅ Comprehensive test suite (79% coverage)

### Remaining Implementation Tasks

- [ ] **Phase 1: Critical Fixes & Improvements**
  - [x] Fix import duplication - only include imports used in each file (COMPLETED in v0.3.0)
  - [ ] Preserve decorators and docstrings properly in extracted files
  - [ ] Handle comments between definitions (currently lost)
  - [ ] Add proper encoding handling for Unicode files
  - [ ] Improve error recovery - partial processing instead of skipping

- [ ] **Phase 2: Code Quality & Architecture**
  - [ ] Refactor `pyxplod.py` into multiple modules:
    - `ast_utils.py` - AST manipulation functions
    - `file_utils.py` - File discovery and I/O
    - `processors.py` - Processing methods (files/dirs)
    - `cli.py` - CLI interface
  - [ ] Add comprehensive type hints throughout
  - [ ] Implement proper logging patterns per CLAUDE.md
  - [ ] Add integration tests with real Python projects

- [ ] **Phase 3: Essential Features**
  - [ ] Add `--dry-run` flag to preview changes without writing
  - [ ] Support `.pyxplod.toml` configuration file
  - [ ] Handle nested classes and inner functions
  - [ ] Add `--exclude` pattern for skipping files/directories
  - [ ] Implement import optimization for extracted files

- [ ] **Phase 4: Advanced Features**
  - [ ] Add `--format` option to preserve formatting using Black
  - [ ] Support for async function annotations
  - [ ] Handle complex decorator chains
  - [ ] Add `--include-private` flag for private methods/classes
  - [ ] Implement reverse operation (`--method implode`)

- [ ] **Phase 5: Performance & Scalability**
  - [ ] Parallel processing using multiprocessing
  - [ ] Streaming AST processing for large files
  - [ ] Incremental mode - only process changed files
  - [ ] Memory-efficient processing for huge codebases
  - [ ] Progress persistence for resumable operations

- [ ] **Phase 6: Enhanced Testing & Documentation**
  - [ ] Add property-based testing with Hypothesis
  - [ ] Create test suite with popular Python projects
  - [ ] Add performance benchmarks
  - [ ] Generate API documentation with Sphinx
  - [ ] Add visual examples and diagrams to docs

### Technical Decisions

1. **AST vs. Regular Expressions**: Use AST for accurate parsing and modification
2. **Import Style**: Use relative imports for split files to maintain portability
3. **Naming Convention**: Snake_case with deduplication to avoid conflicts
4. **Error Handling**: Fail gracefully, skip problematic files with warnings
5. **Logging**: Verbose mode with loguru for debugging

### Dependencies Required
- `fire` - CLI interface
- `loguru` - Enhanced logging
- `ast` - Python AST parsing (built-in)
- `pathlib` - Path operations (built-in)

### Example Transformations

#### Method: files (default)

Input: `src/utils.py`
```python
import os

class MyClass:
    def method(self):
        pass

def my_function():
    pass
```

Output:
```
output/src/
├── utils.py
├── utils_my_class.py
└── utils_my_function.py
```

#### Method: dirs

Same input produces:
```
output/src/
└── utils/
    ├── __init__.py
    ├── my_class.py
    └── my_function.py
```

Both methods maintain API compatibility - external code using `from src.utils import MyClass` continues to work unchanged.

### Known Issues & Limitations

1. ~~**Import Duplication**: All imports from the original file are copied to every extracted file, even if unused~~ (FIXED in v0.3.0)
2. **Lost Elements**: Comments between definitions and some formatting are not preserved
3. **Limited Scope**: Only handles top-level classes and functions, not nested definitions
4. **Formatting**: Uses `ast.unparse()` which doesn't preserve original code formatting
5. **Memory Usage**: Entire AST is kept in memory, could be problematic for very large files
6. **Error Handling**: Files with syntax errors are skipped entirely instead of partial processing

### Design Decisions & Rationale

1. **Two Methods Approach**: 
   - `files` method: Simple, flat structure, good for small modules
   - `dirs` method: Package structure, better for larger modules, cleaner imports

2. **AST-based Processing**: Ensures syntactic correctness and proper Python structure

3. **Import Strategy**: Currently copies all imports for safety, needs optimization

4. **Naming Convention**: Snake_case with deduplication ensures filesystem compatibility

5. **Error Philosophy**: Fail safely, skip problematic files with clear error messages
</file>

<file path="src/pyxplod/pyxplod.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "loguru", "rich"]
# ///
# this_file: src/pyxplod/pyxplod.py

"""pyxplod: Python code exploder - extracts classes and functions into separate files.

This tool takes a Python project and "explodes" it by extracting each class and function
definition into its own file, replacing the original definitions with imports.
"""

import ast
import re
import shutil
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from loguru import logger
from rich.console import Console
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn

console = Console()


def to_snake_case(name: str) -> str:
    """Convert a name to snake_case format.

    Handles CamelCase, pascalCase, and already snake_case names.
    """
    # Insert underscores before uppercase letters that follow lowercase letters
    s1 = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", name)
    # Insert underscores before uppercase letters that follow lowercase or uppercase letters
    s2 = re.sub("([a-z0-9])([A-Z])", r"\1_\2", s1)
    # Convert to lowercase
    return s2.lower()


def extract_imports(tree: ast.AST) -> list[ast.stmt]:
    """Extract all import statements from an AST.

    Returns a list of Import and ImportFrom nodes at module level only.
    """
    imports = []
    for node in tree.body:
        if isinstance(node, ast.Import | ast.ImportFrom):
            imports.append(node)
    return imports


def analyze_name_usage(node: ast.AST) -> set[str]:
    """Analyze which names are used in an AST node.

    Returns a set of all names referenced in the node, including decorators.
    """
    names = set()

    class NameCollector(ast.NodeVisitor):
        def visit_Name(self, node: ast.Name) -> None:
            names.add(node.id)
            self.generic_visit(node)

        def visit_Attribute(self, node: ast.Attribute) -> None:
            # For attributes like os.path, we want 'os'
            if isinstance(node.value, ast.Name):
                names.add(node.value.id)
            self.generic_visit(node)

    # First check for decorators on the node itself
    if hasattr(node, "decorator_list"):
        for decorator in node.decorator_list:
            # Handle simple decorators like @my_decorator
            if isinstance(decorator, ast.Name):
                names.add(decorator.id)
            # Handle attribute decorators like @module.decorator
            elif isinstance(decorator, ast.Attribute) and isinstance(decorator.value, ast.Name):
                names.add(decorator.value.id)
            # For complex decorators, visit them
            NameCollector().visit(decorator)

    # Then collect names from the rest of the node
    NameCollector().visit(node)
    return names


def filter_imports_for_names(imports: list[ast.stmt], used_names: set[str]) -> list[ast.stmt]:
    """Filter imports to only include those that are used.

    Args:
        imports: List of import statements
        used_names: Set of names used in the code

    Returns:
        List of imports that are actually used
    """
    needed_imports = []

    for imp in imports:
        if isinstance(imp, ast.Import):
            # For 'import x, y, z', check each name
            needed_aliases = []
            for alias in imp.names:
                # The name used in code is either the alias or the module name
                name_in_code = alias.asname if alias.asname else alias.name
                # For module.submodule, we check the first part
                base_name = name_in_code.split(".")[0]
                if base_name in used_names:
                    needed_aliases.append(alias)

            if needed_aliases:
                # Create a new import with only needed names
                new_import = ast.Import(names=needed_aliases)
                ast.copy_location(new_import, imp)
                needed_imports.append(new_import)

        elif isinstance(imp, ast.ImportFrom):
            # For 'from x import y, z', check each imported name
            needed_aliases = []
            for alias in imp.names:
                name_in_code = alias.asname if alias.asname else alias.name
                if name_in_code in used_names:
                    needed_aliases.append(alias)

            if needed_aliases:
                # Create a new import with only needed names
                new_import = ast.ImportFrom(module=imp.module, names=needed_aliases, level=imp.level)
                ast.copy_location(new_import, imp)
                needed_imports.append(new_import)

    return needed_imports


def find_definitions(tree: ast.AST) -> list[tuple[ast.stmt, str, str]]:
    """Find all class and function definitions at module level.

    Returns list of tuples: (node, type, name) where type is 'class' or 'function'.
    """
    definitions = []
    for node in tree.body:
        if isinstance(node, ast.ClassDef):
            definitions.append((node, "class", node.name))
        elif isinstance(node, ast.FunctionDef):
            definitions.append((node, "function", node.name))
    return definitions


def generate_filename(base_name: str, def_name: str, def_type: str, existing_files: set) -> str:
    """Generate a unique filename for the extracted definition.

    Handles deduplication by appending numbers if necessary.
    """
    snake_name = to_snake_case(def_name)
    filename = f"{base_name}_{snake_name}.py"

    # Handle deduplication
    if filename in existing_files:
        counter = 2
        while f"{base_name}_{snake_name}_{counter}.py" in existing_files:
            counter += 1
        filename = f"{base_name}_{snake_name}_{counter}.py"

    existing_files.add(filename)
    return filename


def create_import_statement(module_path: str, name: str) -> ast.ImportFrom:
    """Create an import statement for the extracted definition."""
    return ast.ImportFrom(
        module=module_path,
        names=[ast.alias(name=name, asname=None)],
        level=0,  # Absolute import from module
    )


def write_extracted_file(output_path: Path, imports: list[ast.stmt], definition: ast.stmt) -> None:
    """Write the extracted definition to a new file with necessary imports."""
    # Analyze which names are actually used in the definition
    used_names = analyze_name_usage(definition)

    # Filter imports to only include those that are used
    filtered_imports = filter_imports_for_names(imports, used_names)

    # Create a new module with filtered imports and the definition
    new_module = ast.Module(body=[*filtered_imports, definition], type_ignores=[])

    # Generate Python code from AST
    code = ast.unparse(new_module)

    # Write to file with UTF-8 encoding
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(code, encoding="utf-8")
    logger.debug(f"Created file: {output_path} with {len(filtered_imports)} imports (filtered from {len(imports)})")


def process_python_file(input_file: Path, output_base: Path, input_root: Path) -> None:
    """Process a single Python file, extracting definitions and creating new files."""
    logger.info(f"Processing: {input_file}")

    # Calculate relative path structure
    relative_path = input_file.relative_to(input_root)
    output_dir = output_base / relative_path.parent
    output_dir.mkdir(parents=True, exist_ok=True)

    # Read and parse the file
    try:
        content = input_file.read_text(encoding="utf-8")
        tree = ast.parse(content, filename=str(input_file))
    except SyntaxError as e:
        logger.error(f"Syntax error in {input_file}: {e}")
        return
    except Exception as e:
        logger.error(f"Error reading {input_file}: {e}")
        return

    # Extract imports and definitions
    imports = extract_imports(tree)
    definitions = find_definitions(tree)

    if not definitions:
        # No definitions to extract, just copy the file
        output_file = output_base / relative_path
        output_file.write_text(content, encoding="utf-8")
        logger.debug(f"No definitions found, copied: {input_file}")
        return

    # Track created files for deduplication
    existing_files = set()
    base_name = input_file.stem

    # Process each definition
    new_imports = []
    remaining_body = []

    for node in tree.body:
        is_definition = False

        for def_node, def_type, def_name in definitions:
            if node is def_node:
                is_definition = True

                # Generate filename for extracted definition
                filename = generate_filename(base_name, def_name, def_type, existing_files)

                # Write extracted file
                extracted_path = output_dir / filename
                write_extracted_file(extracted_path, imports.copy(), def_node)

                # Create import statement
                import_stmt = create_import_statement(f".{filename[:-3]}", def_name)
                new_imports.append(import_stmt)
                break

        if not is_definition and node not in imports:
            remaining_body.append(node)

    # Create the modified main file
    modified_tree = ast.Module(body=imports + new_imports + remaining_body, type_ignores=tree.type_ignores)

    # Write the modified file
    output_file = output_base / relative_path
    output_file.write_text(ast.unparse(modified_tree), encoding="utf-8")
    logger.info(f"Modified main file: {output_file}")
    logger.debug(f"Extracted {len(definitions)} definitions from {input_file}")


def process_python_file_dirs(input_file: Path, output_base: Path, input_root: Path) -> None:
    """Process a single Python file using the 'dirs' method.

    Creates a directory for each .py file and extracts definitions into separate files
    within that directory, with an __init__.py containing imports and module-level code.
    """
    logger.info(f"Processing (dirs): {input_file}")

    # Calculate relative path structure
    relative_path = input_file.relative_to(input_root)
    # Create directory name from filename (without .py extension)
    dir_name = relative_path.stem
    output_dir = output_base / relative_path.parent / dir_name
    output_dir.mkdir(parents=True, exist_ok=True)

    # Read and parse the file
    try:
        content = input_file.read_text(encoding="utf-8")
        tree = ast.parse(content, filename=str(input_file))
    except SyntaxError as e:
        logger.error(f"Syntax error in {input_file}: {e}")
        return
    except Exception as e:
        logger.error(f"Error reading {input_file}: {e}")
        return

    # Extract imports and definitions
    imports = extract_imports(tree)
    definitions = find_definitions(tree)

    if not definitions:
        # No definitions to extract, create __init__.py with original content
        init_file = output_dir / "__init__.py"
        init_file.write_text(content, encoding="utf-8")
        logger.debug(f"No definitions found, created __init__.py: {input_file}")
        return

    # Track created files for deduplication
    existing_files = set()

    # Process each definition
    new_imports = []
    remaining_body = []

    for node in tree.body:
        is_definition = False

        for def_node, def_type, def_name in definitions:
            if node is def_node:
                is_definition = True

                # Generate filename without prefix for dirs method
                snake_name = to_snake_case(def_name)
                filename = f"{snake_name}.py"

                # Handle deduplication
                if filename in existing_files:
                    counter = 2
                    while f"{snake_name}_{counter}.py" in existing_files:
                        counter += 1
                    filename = f"{snake_name}_{counter}.py"

                existing_files.add(filename)

                # Write extracted file
                extracted_path = output_dir / filename
                write_extracted_file(extracted_path, imports.copy(), def_node)

                # Create import statement for __init__.py
                import_stmt = create_import_statement(f".{filename[:-3]}", def_name)
                new_imports.append(import_stmt)
                break

        if not is_definition and node not in imports:
            remaining_body.append(node)

    # Create __init__.py with imports and remaining code
    init_tree = ast.Module(body=imports + new_imports + remaining_body, type_ignores=tree.type_ignores)

    # Write __init__.py
    init_file = output_dir / "__init__.py"
    init_file.write_text(ast.unparse(init_tree), encoding="utf-8")
    logger.info(f"Created package: {output_dir}")
    logger.debug(f"Extracted {len(definitions)} definitions from {input_file}")


def find_python_files(directory: Path) -> list[Path]:
    """Recursively find all Python files in a directory."""
    python_files = []
    for file in directory.rglob("*.py"):
        # Skip __pycache__ and other Python metadata
        if "__pycache__" not in str(file) and ".pyc" not in str(file):
            python_files.append(file)
    return sorted(python_files)


def validate_paths(input_path: Path, output_path: Path) -> bool:
    """Validate input and output paths."""
    if not input_path.exists():
        logger.error(f"Input path does not exist: {input_path}")
        return False

    if not input_path.is_dir():
        logger.error(f"Input path is not a directory: {input_path}")
        return False

    if output_path.exists() and not output_path.is_dir():
        logger.error(f"Output path exists but is not a directory: {output_path}")
        return False

    return True


def main(input: str, output: str, method: str = "files", verbose: bool = False) -> None:
    """Explode a Python project by extracting classes and functions into separate files.

    Args:
        input: Path to the input directory containing Python files
        output: Path to the output directory where exploded files will be created
        method: Explosion method - 'files' (default) or 'dirs'
        verbose: Enable verbose logging for debugging
    """
    # Validate method parameter
    if method not in ["files", "dirs"]:
        logger.error(f"Invalid method '{method}'. Must be 'files' or 'dirs'.")
        return

    # Configure logging
    if verbose:
        logger.remove()
        logger.add(console.print, format="{time:HH:mm:ss} | {level} | {message}", level="DEBUG")
    else:
        logger.remove()
        logger.add(console.print, format="{message}", level="INFO")

    # Convert to Path objects
    input_path = Path(input).resolve()
    output_path = Path(output).resolve()

    # Validate paths
    if not validate_paths(input_path, output_path):
        return

    # Find all Python files
    python_files = find_python_files(input_path)

    if not python_files:
        logger.warning(f"No Python files found in {input_path}")
        return

    logger.info(f"Found {len(python_files)} Python files to process")

    # Create output directory if it doesn't exist
    output_path.mkdir(parents=True, exist_ok=True)

    # Process each file with progress bar
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        console=console,
    ) as progress:
        task = progress.add_task("Processing files...", total=len(python_files))

        for py_file in python_files:
            try:
                if method == "files":
                    process_python_file(py_file, output_path, input_path)
                else:  # method == "dirs"
                    process_python_file_dirs(py_file, output_path, input_path)
                progress.update(task, advance=1)
            except Exception as e:
                logger.error(f"Failed to process {py_file}: {e}")
                if verbose:
                    logger.exception("Detailed error:")

    logger.info(f"✨ Successfully exploded {len(python_files)} files to {output_path} using method '{method}'")
</file>

<file path="tests/test_package.py">
"""Test suite for pyxplod."""


def test_version():
    """Verify package exposes version."""
    import pyxplod

    assert pyxplod.__version__
</file>

<file path=".gitignore">
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private
VERSION.txt
# SpecStory explanation file
.specstory/.what-is-this.md
</file>

<file path="CLAUDE.md">
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</file>

<file path="pyproject.toml">
[project]
name = 'pyxplod'
description = ''
readme = 'README.md'
requires-python = '>=3.10'
keywords = []
dynamic = ['version']
classifiers = [
    'Development Status :: 4 - Beta',
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
    'Operating System :: OS Independent',
    'License :: OSI Approved :: MIT License',
    'Intended Audience :: Developers',
]
dependencies = [
    'fire>=0.7.0',
    'loguru>=0.7.2',
    'rich>=13.8.0',
]

[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

[project.license]
text = 'MIT'

[project.urls]
Documentation = 'https://github.com/twardoch/pyxplod#readme'
Issues = 'https://github.com/twardoch/pyxplod/issues'
Source = 'https://github.com/twardoch/pyxplod'

[project.optional-dependencies]
dev = [
    'pre-commit>=4.1.0',
    'ruff>=0.9.7',
    'mypy>=1.15.0',
    'absolufy-imports>=0.3.1',
    'pyupgrade>=3.19.1',
    'isort>=6.0.1',
]
test = [
    'pytest>=8.3.4',
    'pytest-cov>=6.0.0',
    'pytest-xdist>=3.6.1',
    'pytest-benchmark[histogram]>=5.1.0',
    'pytest-asyncio>=0.25.3',
    'coverage[toml]>=7.6.12',
]
docs = [
    'sphinx>=7.2.6',
    'sphinx-rtd-theme>=2.0.0',
    'sphinx-autodoc-typehints>=2.0.0',
    'myst-parser>=3.0.0',
    'pytest>=8.3.4',
    'pytest-cov>=6.0.0',
    'pytest-xdist>=3.6.1',
    'pytest-benchmark[histogram]>=5.1.0',
    'pytest-asyncio>=0.25.3',
    'coverage[toml]>=7.6.12',
]
all = [
    'absolufy-imports>=0.3.1',
    'fire>=0.7.0',
    'hatch-vcs>=0.4.0',
    'hatchling>=1.27.0',
    'isort>=6.0.1',
    'loguru>=0.7.2',
    'mypy>=1.15.0',
    'pre-commit>=4.1.0',
    'pyupgrade>=3.19.1',
    'rich>=13.8.0',
    'ruff>=0.9.7',
    'myst-parser>=3.0.0',
    'sphinx-autodoc-typehints>=2.0.0',
    'sphinx-rtd-theme>=2.0.0',
    'sphinx>=7.2.6',
]

[project.scripts]
pyxplod = 'pyxplod.pyxplod:main'

[build-system]
requires = [
    'hatchling>=1.27.0',
    'hatch-vcs>=0.4.0',
]
build-backend = 'hatchling.build'
[tool.hatch.build]
include = [
    'src/pyxplod/py.typed',
    'src/pyxplod/data/**/*',
]
exclude = [
    '**/__pycache__',
    '**/.pytest_cache',
    '**/.mypy_cache',
]
[tool.hatch.build.targets.wheel]
packages = ['src/pyxplod']
reproducible = true
[tool.hatch.build.hooks.vcs]
version-file = 'src/pyxplod/__version__.py'

[tool.hatch.version]
source = 'vcs'

[tool.hatch.metadata]
allow-direct-references = true
[tool.hatch.envs.default]
features = [
    'dev',
    'test',
    'all',
]
dependencies = [
    'fire>=0.7.0',
    'loguru>=0.7.2',
    'rich>=13.8.0',
]

[tool.hatch.envs.default.scripts]
test = 'pytest {args:tests}'
test-cov = 'pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/pyxplod --cov=tests {args:tests}'
type-check = 'mypy src/pyxplod tests'
lint = [
    'ruff check src/pyxplod tests',
    'ruff format --respect-gitignore src/pyxplod tests',
]
fmt = [
    'ruff format --respect-gitignore src/pyxplod tests',
    'ruff check --fix src/pyxplod tests',
]
fix = [
    'ruff check --fix --unsafe-fixes src/pyxplod tests',
    'ruff format --respect-gitignore src/pyxplod tests',
]
[[tool.hatch.envs.all.matrix]]
python = [
    '3.10',
    '3.11',
    '3.12',
]

[tool.hatch.envs.lint]
detached = true
features = ['dev']

[tool.hatch.envs.lint.scripts]
typing = 'mypy --install-types --non-interactive {args:src/pyxplod tests}'
style = [
    'ruff check {args:.}',
    'ruff format --respect-gitignore {args:.}',
]
fmt = [
    'ruff format --respect-gitignore {args:.}',
    'ruff check --fix {args:.}',
]
fix = [
    'ruff check --fix --unsafe-fixes {args:.}',
    'ruff format --respect-gitignore {args:.}',
]
all = [
    'style',
    'typing',
    'fix',
]

[tool.hatch.envs.test]
features = ['test']

[tool.hatch.envs.test.scripts]
test = 'python -m pytest -n auto {args:tests}'
test-cov = 'python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/pyxplod --cov=tests {args:tests}'
bench = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only'
bench-save = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json'

[tool.hatch.envs.docs]
features = ['docs']

[tool.hatch.envs.docs.scripts]
build = 'sphinx-build -b html docs/source docs/build'

[tool.hatch.envs.ci]
features = ['test']

[tool.hatch.envs.ci.scripts]
test = 'pytest --cov=src/pyxplod --cov-report=xml'
[tool.coverage.paths]
pyxplod = [
    'src/pyxplod',
    '*/pyxplod/src/pyxplod',
]
tests = [
    'tests',
    '*/pyxplod/tests',
]

[tool.coverage.report]
exclude_lines = [
    'no cov',
    'if __name__ == .__main__.:',
    'if TYPE_CHECKING:',
    'pass',
    'raise NotImplementedError',
    'raise ImportError',
    'except ImportError',
    'except KeyError',
    'except AttributeError',
    'except NotImplementedError',
]

[tool.coverage.run]
source_pkgs = [
    'pyxplod',
    'tests',
]
branch = true
parallel = true
omit = ['src/pyxplod/__about__.py']

[tool.mypy]
python_version = '3.10'
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[[tool.mypy.overrides]]
module = ['tests.*']
disallow_untyped_defs = false
disallow_incomplete_defs = false
[tool.pytest.ini_options]
addopts = '-v --durations=10 -p no:briefcase'
asyncio_mode = 'auto'
asyncio_default_fixture_loop_scope = 'function'
console_output_style = 'progress'
filterwarnings = [
    'ignore::DeprecationWarning',
    'ignore::UserWarning',
]
log_cli = true
log_cli_level = 'INFO'
markers = [
    '''benchmark: marks tests as benchmarks (select with '-m benchmark')''',
    'unit: mark a test as a unit test',
    'integration: mark a test as an integration test',
    'permutation: tests for permutation functionality',
    'parameter: tests for parameter parsing',
    'prompt: tests for prompt parsing',
]
norecursedirs = [
    '.*',
    'build',
    'dist',
    'venv',
    '__pycache__',
    '*.egg-info',
    '_private',
]
python_classes = ['Test*']
python_files = ['test_*.py']
python_functions = ['test_*']
testpaths = ['tests']

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = 'file'
save-data = true
compare = [
    'min',
    'max',
    'mean',
    'stddev',
    'median',
    'iqr',
    'ops',
    'rounds',
]

[tool.ruff]
target-version = 'py310'
line-length = 120

[tool.ruff.lint]
select = [
    'A',
    'ARG',
    'ASYNC',
    'B',
    'C',
    'DTZ',
    'E',
    'EM',
    'F',
    'FBT',
    'I',
    'ICN',
    'ISC',
    'LOG',
    'N',
    'PLC',
    'PLE',
    'PLR',
    'PLW',
    'PT',
    'PTH',
    'PYI',
    'RET',
    'RSE',
    'RUF',
    'S',
    'SIM',
    'T',
    'TCH',
    'TID',
    'UP',
    'W',
    'YTT',
]
ignore = [
    'B027',
    'C901',
    'FBT003',
    'PLR0911',
    'PLR0912',
    'PLR0913',
    'PLR0915',
    'PLR1714',
    'PLW0603',
    'PT013',
    'PTH123',
    'PYI056',
    'S105',
    'S106',
    'S107',
    'S110',
    'SIM102',
]
unfixable = ['F401']
exclude = [
    '.git',
    '.venv',
    'venv',
    'dist',
    'build',
    '__pycache__',
]

[tool.ruff.lint.isort]
known-first-party = ['pyxplod']

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = 'all'

[tool.ruff.lint.per-file-ignores]
"tests/**/*" = [
    'PLR2004',
    'S101',
    'TID252',
]
</file>

<file path="README.md">
# pyxplod

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**pyxplod** is a Python refactoring tool that "explodes" Python files by extracting each class and function definition into separate files, automatically replacing them with imports. This helps break down large Python modules into smaller, more manageable pieces while maintaining functionality.

## Features

- 🔍 **Intelligent code extraction**: Uses Python's AST to accurately identify and extract classes and functions
- 📁 **Structure preservation**: Maintains your project's directory structure in the output
- 🔗 **Automatic imports**: Replaces extracted code with proper relative imports
- 🛡️ **Safe operation**: Non-destructive - creates new files without modifying originals
- 📊 **Progress tracking**: Visual progress bars and detailed logging
- 🎯 **Smart naming**: Converts class/function names to snake_case for consistent file naming
- ⚡ **Fast processing**: Efficiently handles large codebases

## Installation

```bash
pip install pyxplod
```

Or using `uv`:

```bash
uv pip install pyxplod
```

## Usage

### Basic Usage (Files Method)

```bash
pyxplod --input /path/to/source --output /path/to/output
```

### Using Dirs Method

```bash
pyxplod --input /path/to/source --output /path/to/output --method dirs
```

### With Verbose Logging

```bash
pyxplod --input /path/to/source --output /path/to/output --verbose
```

## Methods

pyxplod supports two different explosion methods:

### Files Method (Default)

The `files` method creates separate files in the same directory structure, with each extracted class/function having a filename prefix based on the original file.

### Dirs Method

The `dirs` method creates a directory (package) for each Python file, with extracted classes and functions as separate modules within that package. An `__init__.py` file contains imports and any remaining module-level code.

## How It Works

Given a Python file like this:

```python
# src/utils.py
import os
from typing import List

class FileHandler:
    def __init__(self):
        self.files = []
    
    def add_file(self, path: str):
        self.files.append(path)

def process_data(data: List[str]) -> str:
    return "\n".join(data)

CONSTANT = "some_value"
```

pyxplod will create different structures based on the method used:

### Files Method Output (Default)

```
output/
└── src/
    ├── utils.py                    # Modified main file
    ├── utils_file_handler.py       # Extracted class
    └── utils_process_data.py       # Extracted function
```

### Dirs Method Output

```
output/
└── src/
    └── utils/
        ├── __init__.py             # Module interface with imports
        ├── file_handler.py         # Extracted class
        └── process_data.py         # Extracted function
```

## Example Contents

**output/src/utils.py:**
```python
import os
from typing import List
from .utils_file_handler import FileHandler
from .utils_process_data import process_data

CONSTANT = "some_value"
```

**output/src/utils_file_handler.py:**
```python
import os
from typing import List

class FileHandler:
    def __init__(self):
        self.files = []
    
    def add_file(self, path: str):
        self.files.append(path)
```

**output/src/utils_process_data.py:**
```python
import os
from typing import List

def process_data(data: List[str]) -> str:
    return "\n".join(data)
```

## Use Cases

- **Refactoring large modules**: Break down monolithic Python files into smaller, focused modules
- **Code organization**: Improve project structure by separating concerns
- **Testing**: Make it easier to test individual components in isolation
- **Code review**: Simplify code reviews by creating smaller, single-purpose files
- **Legacy code**: Gradually modernize legacy codebases by extracting components

## Features in Detail

### Smart Import Handling
- Preserves all module-level imports in extracted files
- Generates relative imports for the extracted components
- Maintains import order and structure

### Error Handling
- Gracefully handles syntax errors in source files
- Skips files that cannot be parsed
- Provides detailed error messages for troubleshooting

### File Naming
- Converts CamelCase to snake_case automatically
- Handles naming conflicts with automatic deduplication
- Preserves meaningful names while ensuring filesystem compatibility

## Development

### Setup Development Environment

```bash
# Clone the repository
git clone https://github.com/twardoch/pyxplod.git
cd pyxplod

# Install in development mode
pip install -e .

# Install development dependencies
pip install -e ".[dev,test]"
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=pyxplod

# Run specific test
pytest tests/test_pyxplod.py::TestProcessing::test_process_simple_file
```

### Code Quality

```bash
# Format code
black src/ tests/

# Run linting
ruff check src/ tests/

# Type checking
mypy src/
```

## Requirements

- Python 3.10 or higher
- No external dependencies for core functionality

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Changelog

See [CHANGELOG.md](CHANGELOG.md) for a detailed history of changes.

## Author

Created by Adam Twardoch
</file>

<file path="TODO.md">
# TODO

## High Priority Fixes

- [ ] **Preserve code elements properly** (CRITICAL)
  - [ ] Maintain decorators on extracted functions/classes
  - [ ] Preserve docstrings in correct positions
  - [ ] Handle comments between definitions
  - [ ] Keep type annotations intact

- [ ] **Improve error handling**
  - [ ] Add proper Unicode/encoding support (UTF-8)
  - [ ] Implement partial file processing on errors
  - [ ] Better error messages with line numbers
  - [ ] Add --skip-errors flag

## Code Quality Improvements

- [ ] **Refactor into modules** (currently 450+ lines in one file)
  - [ ] `ast_utils.py` - AST helper functions
  - [ ] `file_utils.py` - File operations
  - [ ] `processors.py` - Method implementations  
  - [ ] `cli.py` - Command interface

- [ ] **Enhance type hints**
  - [ ] Use proper AST types instead of generic types
  - [ ] Add return type hints to all functions
  - [ ] Use simpler union syntax per CLAUDE.md

- [ ] **Add comprehensive logging**
  - [ ] Debug log all AST operations
  - [ ] Log import analysis decisions
  - [ ] Add --debug flag for extra verbosity

## Essential Features

- [ ] **Add --dry-run mode**
  - Show what would be changed without writing files
  - Display file tree of output
  - Report statistics (files, classes, functions)

- [ ] **Configuration file support**
  - [ ] Create `.pyxplod.toml` spec
  - [ ] Support exclude/include patterns
  - [ ] Method preferences
  - [ ] Output formatting options

- [ ] **Handle complex code structures**
  - [ ] Nested classes and inner functions
  - [ ] Async functions and decorators
  - [ ] Class methods and static methods
  - [ ] Property decorators

## Testing Improvements

- [ ] **Add missing test cases**
  - [x] Test import optimization (basic coverage added)
  - [ ] Test decorator preservation
  - [ ] Test Unicode file handling
  - [ ] Test large file processing
  - [ ] Test error conditions

- [ ] **Integration tests**
  - [ ] Test with popular packages (requests, flask, etc.)
  - [ ] Cross-platform path handling
  - [ ] Performance benchmarks

## Documentation

- [ ] **Improve inline documentation**
  - [ ] Add comments explaining AST operations
  - [ ] Document import analysis logic
  - [ ] Explain the two-method architecture

- [ ] **Add examples directory**
  - [ ] Before/after examples for both methods
  - [ ] Complex code structure examples
  - [ ] Configuration file examples

## Future Enhancements

- [ ] **Performance optimizations**
  - [ ] Parallel file processing
  - [ ] Streaming for large files
  - [ ] Caching for incremental updates

- [ ] **Advanced features**
  - [ ] `--format` flag with Black integration
  - [ ] `--verify` flag to check output validity
  - [ ] Plugin system for custom processors
  - [ ] Reverse operation (implode/merge)

## Recently Completed (v0.3.0)

- [x] **Fix import duplication bug** - Implemented smart import filtering
  - Added `analyze_name_usage()` to detect which names are used in code
  - Added `filter_imports_for_names()` to only include necessary imports
  - Reduced extracted file sizes by 30-50%
  - Enhanced logging to show import filtering statistics

## Recently Completed (v0.2.0)

- [x] Write professional README.md
- [x] Add dirs method specification to PLAN.md
- [x] Implement --method dirs functionality
- [x] Add tests for both methods
- [x] Update documentation for both methods
</file>

</files>
.
├── AGENT.md
├── CHANGELOG.md
├── CLAUDE.md
├── dist
├── LICENSE
├── LLM.txt
├── package.toml
├── PLAN.md
├── pyproject.toml
├── README.md
├── src
│   └── pyxplod
│       ├── __init__.py
│       ├── __main__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-312.pyc
│       │   ├── __main__.cpython-312.pyc
│       │   ├── __version__.cpython-312.pyc
│       │   └── pyxplod.cpython-312.pyc
│       ├── __version__.py
│       └── pyxplod.py
├── src-dirs
│   └── pyxplod
│       ├── __init__
│       │   └── __init__.py
│       ├── __main__
│       │   └── __init__.py
│       ├── __version__
│       │   └── __init__.py
│       └── pyxplod
│           ├── __init__.py
│           ├── analyze_name_usage.py
│           ├── create_import_statement.py
│           ├── extract_imports.py
│           ├── filter_imports_for_names.py
│           ├── find_definitions.py
│           ├── find_python_files.py
│           ├── generate_filename.py
│           ├── main.py
│           ├── process_python_file_dirs.py
│           ├── process_python_file.py
│           ├── to_snake_case.py
│           ├── validate_paths.py
│           └── write_extracted_file.py
├── src-files
│   └── pyxplod
│       ├── __init__.py
│       ├── __main__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-312.pyc
│       │   ├── __main__.cpython-312.pyc
│       │   ├── __version__.cpython-312.pyc
│       │   ├── pyxplod_analyze_name_usage.cpython-312.pyc
│       │   ├── pyxplod_create_import_statement.cpython-312.pyc
│       │   ├── pyxplod_extract_imports.cpython-312.pyc
│       │   ├── pyxplod_filter_imports_for_names.cpython-312.pyc
│       │   ├── pyxplod_find_definitions.cpython-312.pyc
│       │   ├── pyxplod_find_python_files.cpython-312.pyc
│       │   ├── pyxplod_generate_filename.cpython-312.pyc
│       │   ├── pyxplod_main.cpython-312.pyc
│       │   ├── pyxplod_process_python_file_dirs.cpython-312.pyc
│       │   ├── pyxplod_process_python_file.cpython-312.pyc
│       │   ├── pyxplod_to_snake_case.cpython-312.pyc
│       │   ├── pyxplod_validate_paths.cpython-312.pyc
│       │   ├── pyxplod_write_extracted_file.cpython-312.pyc
│       │   └── pyxplod.cpython-312.pyc
│       ├── __version__.py
│       ├── pyxplod_analyze_name_usage.py
│       ├── pyxplod_create_import_statement.py
│       ├── pyxplod_extract_imports.py
│       ├── pyxplod_filter_imports_for_names.py
│       ├── pyxplod_find_definitions.py
│       ├── pyxplod_find_python_files.py
│       ├── pyxplod_generate_filename.py
│       ├── pyxplod_main.py
│       ├── pyxplod_process_python_file_dirs.py
│       ├── pyxplod_process_python_file.py
│       ├── pyxplod_to_snake_case.py
│       ├── pyxplod_validate_paths.py
│       ├── pyxplod_write_extracted_file.py
│       └── pyxplod.py
├── test_unicode.py
├── tests
│   ├── __pycache__
│   │   ├── test_package.cpython-312-pytest-8.3.5.pyc
│   │   └── test_pyxplod.cpython-312-pytest-8.3.5.pyc
│   ├── test_package.py
│   └── test_pyxplod.py
└── TODO.md

16 directories, 74 files
</file>

<file path="package.toml">
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(python -m pytest tests/ -v)",
      "Bash(uv pip install:*)",
      "Bash(python -m pytest tests/test_pyxplod.py -v)",
      "Bash(python -m pytest tests/test_pyxplod.py::TestProcessing::test_process_simple_file -xvs)",
      "Bash(python -m pytest tests/ -v --tb=short)",
      "Bash(black:*)",
      "Bash(pyxplod:*)",
      "Bash(python:*)",
      "Bash(rm:*)",
      "Bash(tree:*)",
      "Bash(cat:*)",
      "Bash(echo:*)",
      "Bash(find:*)",
      "Bash(mkdir:*)",
      "Bash(ls:*)",
      "Bash(grep:*)",
      "Bash(rg:*)"
    ],
    "deny": []
  }
}
</file>

<file path=".cursor/rules/frame-fingerprinting-algorithm.mdc">
---
description: Frame fingerprinting algorithms for video frame comparison and matching using perceptual hashing techniques
globs: **/frame_fingerprint.py,**/hash/*.py,**/video/comparison.py,**/fingerprint/*.py
alwaysApply: false
---


# frame-fingerprinting-algorithm

The frame fingerprinting system uses multiple perceptual hashing algorithms to generate compact, comparable representations of video frames.

### Core Fingerprinting Components (Importance: 95)

The FrameFingerprinter module combines four distinct perceptual hashing approaches:

1. pHash (Perceptual Hash)
- Operates in frequency domain using DCT transformation
- Robust against brightness, contrast and gamma variations
- Returns binary hash sensitive to image structure 

2. AverageHash
- Computes frame's average color values
- Returns simplified binary representation
- Fast but less precise than pHash

3. ColorMomentHash 
- Captures statistical color distribution data
- Stores color moment features as hash
- Effective for content with distinct color patterns

4. MarrHildrethHash
- Edge and shape detection based
- Captures structural frame features
- Robust against color/lighting changes

### Frame Comparison Workflow (Importance: 90)

1. Frame Standardization
- Resize input frames to 64x64 pixels
- Convert to grayscale for consistent processing
- Normalize brightness/contrast

2. Multi-Hash Generation
- Apply all four hash algorithms in parallel
- Generate color histogram
- Combine into unified fingerprint dictionary

3. Fingerprint Comparison
- Calculate normalized Hamming distances between hashes
- Compute histogram correlation
- Weight and combine similarity scores
- Returns similarity value between 0-1

### Integration Points (Importance: 85)

The fingerprinting system interfaces with:

- Temporal alignment for frame sequence matching
- Spatial alignment for template matching
- Video import/export pipeline for frame extraction
- Parallel processing system for hash generation

$END$
</file>

<file path=".cursor/rules/spatial-alignment-algorithm.mdc">
---
description: Technical specification for template matching-based video overlay positioning algorithm
globs: src/vidkompy/spatial.py,src/vidkompy/alignment.py,src/vidkompy/template.py
alwaysApply: false
---


# spatial-alignment-algorithm

The spatial alignment algorithm determines optimal positioning of foreground video frames onto background frames through template matching.

### Frame Selection Strategy
- Extracts representative frames from temporal midpoint of both videos
- Avoids intro/outro sequences that may contain unrepresentative content
- Single frame comparison optimizes processing while maintaining accuracy

### Template Matching Process
1. Frame Preprocessing:
   - Converts frames to grayscale for robust matching
   - Applies cv2.matchTemplate with TM_CCOEFF_NORMED correlation method
   - Calculates normalized cross-correlation scores across all possible positions

2. Position Detection:
   - Identifies highest correlation score location using cv2.minMaxLoc
   - Returns (x_offset, y_offset) coordinates for top-left corner placement
   - Correlation score provides confidence metric for match quality

3. Scale Handling:
   - Checks foreground video dimensions against background
   - Automatically scales down oversized foreground content
   - Records scale factor for consistent frame processing

4. Border Mode:
   - Optional border-based matching for content alignment
   - Uses configurable margin parameter (default 8 pixels)
   - Focuses on visible background edges for positioning

Importance Scores:
- Frame Selection: 85 - Critical for representative matching
- Template Matching: 95 - Core positioning algorithm
- Scale Handling: 80 - Essential for content compatibility
- Border Mode: 75 - Important alternative alignment method

$END$
</file>

<file path=".cursor/rules/temporal-alignment-dtw.mdc">
---
description: Specification for implementing temporal alignment between videos using Dynamic Time Warping (DTW) algorithms
globs: **/temporal/*.py,**/alignment/*.py,**/dtw/*.py,**/sync/*.py
alwaysApply: false
---


# temporal-alignment-dtw

The temporal alignment system uses Dynamic Time Warping (DTW) to synchronize foreground and background videos with different timings and frame rates.

## Core Business Logic

### DTW Frame Alignment
- Creates frame-to-frame mapping between foreground and background videos
- Constructs cost matrix representing similarity between all possible frame pairs 
- Applies monotonicity constraint to prevent backwards time jumps
- Uses Sakoe-Chiba band windowing to constrain warping path
- Generates continuous alignment map through interpolation between key frames

### Frame Fingerprinting
- Standardizes frames to uniform size/grayscale for comparison
- Implements multi-algorithm perceptual hashing:
  - pHash (DCT frequency analysis)
  - AverageHash (mean color values)
  - ColorMomentHash (color distribution) 
  - MarrHildrethHash (edge detection)
- Combines hashes with color histograms into unified fingerprint

### Temporal Synchronization Flow
1. Sample frames from both videos at varying densities
2. Generate fingerprints for sampled frames
3. Build frame similarity cost matrix
4. Find optimal DTW path through cost matrix
5. Interpolate full frame mapping from sparse matches

### Drift Prevention 
- Adaptive keyframe sampling based on FPS differences
- Maintains consistent sync without accumulating temporal drift
- Preserves foreground timing while warping background

### Video Composition
- Uses frame mapping to fetch correct background frames
- Ensures sequential frame reading for performance
- Maintains perfect frame-level synchronization

Importance Scores:
- DTW Frame Alignment: 95 (Core synchronization algorithm)
- Frame Fingerprinting: 90 (Critical for frame matching)
- Temporal Flow: 85 (Key workflow orchestration)
- Drift Prevention: 80 (Essential quality control)
- Video Composition: 75 (Integration of aligned content)

$END$
</file>

<file path=".cursor/rules/video-processing-pipeline.mdc">
---
description: Technical specification for video frame processing, synchronization and composition pipeline
globs: src/vidkompy/pipeline/**/*.py,src/vidkompy/core/*.py,src/vidkompy/sync/**/*.py
alwaysApply: false
---


# video-processing-pipeline

## Frame Processing Pipeline (Importance: 95)
The pipeline follows a "foreground-first" principle to preserve content integrity:

1. Frame Extraction & Fingerprinting
- Generates perceptual hashes and color histograms for each frame
- Multiple hash types combined for robust frame matching
- Parallel processing across CPU cores for fingerprint generation

2. Spatial Alignment
- Template matching using normalized cross-correlation
- Determines optimal (x,y) offset for foreground overlay
- Handles automatic scaling if foreground exceeds background dimensions

3. Temporal Synchronization
- Dynamic Time Warping (DTW) creates frame-to-frame mapping
- Enforces monotonicity to prevent timeline jumps
- Interpolates between key matches for smooth transitions

4. Video Composition
- Sequential frame reading for optimized I/O
- Background frames dynamically matched to foreground timeline
- Optional smooth blending at frame edges

## Frame Mapping System (Importance: 90)
Core algorithm for maintaining perfect synchronization:

1. FrameAlignment Structure
- Maps each foreground frame index to corresponding background frame
- Handles framerate mismatches through interpolation
- Maintains global temporal coherence

2. Frame Selection Logic
- Samples more frames from background for flexible matching
- Uses mid-point frame selection for spatial alignment
- Adaptive keyframe density based on FPS differences

## Audio Integration (Importance: 85)
Intelligent audio handling process:

1. Source Selection
- Prioritizes foreground audio track when available
- Falls back to background audio if needed
- Validates audio stream compatibility

2. Synchronization
- Applies temporal offset matching video alignment
- Ensures perfect audio/video sync in final output
- Handles different audio formats and sample rates

$END$
</file>

<file path="src/pyxplod/__init__.py">
# this_file: src/pyxplod/__init__.py
"""pyxplod: Python code exploder - extracts classes and functions into separate files."""

from pyxplod.__version__ import __version__
from pyxplod.cli import main  # Updated import

__all__ = ["__version__", "main"]
</file>

<file path="src/pyxplod/__main__.py">
# this_file: src/pyxplod/__main__.py
"""Entry point for running pyxplod as a module."""

import fire

from pyxplod.cli import main  # Updated import

if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="src/pyxplod/file_utils.py">
# this_file: src/pyxplod/file_utils.py
"""File system and path related utility functions for pyxplod."""

import ast
from pathlib import Path

# For Python 3.9+, list, set, tuple are standard types for hinting.
from loguru import logger

from pyxplod.ast_utils import analyze_name_usage, filter_imports_for_names
from pyxplod.utils import to_snake_case


def generate_filename(base_name: str, def_name: str, existing_files: set) -> str:  # def_type removed
    """Generate a unique filename for the extracted definition.

    Handles deduplication by appending numbers if necessary.
    The def_type argument was previously unused.
    """
    snake_name = to_snake_case(def_name)
    filename = f"{base_name}_{snake_name}.py"

    # Handle deduplication
    if filename in existing_files:
        counter = 2
        while f"{base_name}_{snake_name}_{counter}.py" in existing_files:
            counter += 1
        filename = f"{base_name}_{snake_name}_{counter}.py"

    existing_files.add(filename)
    return filename


def write_extracted_file(
    output_path: Path,
    imports: list[ast.stmt],
    definition: ast.stmt,
    module_variables: list[tuple[ast.stmt, str]] | None = None,
) -> None:
    """Write the extracted definition to a new file with necessary imports and module variables."""
    if module_variables is None:
        module_variables = []

    # Analyze which names are actually used in the definition
    used_names = analyze_name_usage(definition)

    # Find which module variables are needed by this definition
    needed_variables = []
    # variable_names = set() # This variable was unused
    for var_node, var_name in module_variables:
        if var_name in used_names:
            needed_variables.append(var_node)
            # variable_names.add(var_name) # This variable was unused
            # Also analyze names used in the variable assignment itself
            var_used_names = analyze_name_usage(var_node)
            used_names.update(var_used_names)
            logger.debug(f"Including module variable '{var_name}' in {output_path.name}")

    # Filter imports to include those used by both definition and needed variables
    filtered_imports = filter_imports_for_names(imports, used_names)

    # Create a new module with filtered imports, needed variables, and the definition
    # Order: imports first, then module variables, then definition
    new_module = ast.Module(body=[*filtered_imports, *needed_variables, definition], type_ignores=[])

    # Generate Python code from AST
    code = ast.unparse(new_module)

    # Write to file with UTF-8 encoding
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(code, encoding="utf-8")
    logger.debug(f"Created file: {output_path} with {len(filtered_imports)} imports, {len(needed_variables)} variables")


def find_python_files(directory: Path) -> list[Path]:
    """Recursively find all Python files in a directory."""
    python_files: list[Path] = [
        file
        for file in directory.rglob("*.py")
        if "__pycache__" not in str(file) and ".pyc" not in str(file)
    ]
    return sorted(python_files)


def validate_paths(input_path: Path, output_path: Path) -> bool:
    """Validate input and output paths."""
    if not input_path.exists():
        logger.error(f"Input path does not exist: {input_path}")
        return False

    if not input_path.is_dir():
        logger.error(f"Input path is not a directory: {input_path}")
        return False

    if output_path.exists() and not output_path.is_dir():
        logger.error(f"Output path exists but is not a directory: {output_path}")
        return False

    return True
</file>

<file path=".gitignore">
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private
VERSION.txt
# SpecStory explanation file
.specstory/.what-is-this.md
</file>

<file path="CLAUDE.md">
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</file>

<file path="tests/test_package.py">
"""Test suite for pyxplod."""

import pyxplod


def test_version():
    """Verify package exposes version."""
    assert pyxplod.__version__
</file>

<file path="tests/test_pyxplod.py">
#!/usr/bin/env python3
# this_file: tests/test_pyxplod.py

"""Test suite for pyxplod functionality."""

import ast

from pyxplod.ast_utils import (
    create_import_statement,
    extract_imports,
    find_definitions,
)
from pyxplod.file_utils import (
    find_python_files,
    generate_filename,
    validate_paths,
)
from pyxplod.processors import process_python_file, process_python_file_dirs
from pyxplod.utils import to_snake_case


class TestUtilityFunctions:
    """Test utility functions used in pyxplod."""

    def test_to_snake_case(self):
        """Test conversion of various formats to snake_case."""
        assert to_snake_case("CamelCase") == "camel_case"
        assert to_snake_case("camelCase") == "camel_case"
        assert to_snake_case("snake_case") == "snake_case"
        assert to_snake_case("HTTPResponse") == "http_response"
        assert to_snake_case("getHTTPResponseCode") == "get_http_response_code"
        assert to_snake_case("SimpleTest") == "simple_test"
        assert to_snake_case("a") == "a"
        assert to_snake_case("A") == "a"

    def test_extract_imports(self):
        """Test extraction of import statements from AST."""
        code = """
import os
from pathlib import Path
import sys
from typing import List, Dict

def my_function():
    pass
"""
        tree = ast.parse(code)
        imports = extract_imports(tree)

        assert len(imports) == 4
        assert all(isinstance(imp, ast.Import | ast.ImportFrom) for imp in imports)

    def test_find_definitions(self):
        """Test finding class and function definitions."""
        code = """
import os

class MyClass:
    pass

def my_function():
    pass

class AnotherClass:
    def method(self):
        pass

def another_function():
    return 42

x = 10  # Not a definition
"""
        tree = ast.parse(code)
        definitions = find_definitions(tree)

        assert len(definitions) == 4
        assert definitions[0][1] == "class"
        assert definitions[0][2] == "MyClass"
        assert definitions[1][1] == "function"
        assert definitions[1][2] == "my_function"
        assert definitions[2][1] == "class"
        assert definitions[2][2] == "AnotherClass"
        assert definitions[3][1] == "function"
        assert definitions[3][2] == "another_function"

    def test_generate_filename(self):
        """Test filename generation with deduplication."""
        existing = set()

        # First file
        name1 = generate_filename("module", "MyClass", existing)  # "class" argument removed
        assert name1 == "module_my_class.py"

        # Duplicate should get a number
        name2 = generate_filename("module", "MyClass", existing)  # "class" argument removed
        assert name2 == "module_my_class_2.py"

        # Another duplicate
        name3 = generate_filename("module", "MyClass", existing)  # "class" argument removed
        assert name3 == "module_my_class_3.py"

        # Different name should work normally
        name4 = generate_filename("module", "OtherClass", existing)  # "class" argument removed
        assert name4 == "module_other_class.py"

    def test_create_import_statement(self):
        """Test creation of import statements."""
        import_stmt = create_import_statement(".module_my_class", "MyClass")

        assert isinstance(import_stmt, ast.ImportFrom)
        assert import_stmt.module == ".module_my_class"
        assert import_stmt.level == 0
        assert len(import_stmt.names) == 1
        assert import_stmt.names[0].name == "MyClass"
        assert import_stmt.names[0].asname is None


class TestFileOperations:
    """Test file discovery and validation functions."""

    def test_find_python_files(self, tmp_path):
        """Test recursive Python file discovery."""
        # Create test directory structure
        (tmp_path / "src").mkdir()
        (tmp_path / "src" / "module1.py").write_text("# Python file")
        (tmp_path / "src" / "module2.py").write_text("# Another file")
        (tmp_path / "src" / "subdir").mkdir()
        (tmp_path / "src" / "subdir" / "module3.py").write_text("# Nested file")
        (tmp_path / "src" / "__pycache__").mkdir()
        (tmp_path / "src" / "__pycache__" / "module1.pyc").write_text("# Compiled")
        (tmp_path / "README.md").write_text("# Not a Python file")

        files = find_python_files(tmp_path)

        assert len(files) == 3
        assert all(f.suffix == ".py" for f in files)
        assert all("__pycache__" not in str(f) for f in files)
        assert all(".pyc" not in str(f) for f in files)

    def test_validate_paths(self, tmp_path):
        """Test path validation."""
        # Valid paths
        input_dir = tmp_path / "input"
        input_dir.mkdir()
        output_dir = tmp_path / "output"

        assert validate_paths(input_dir, output_dir) is True

        # Non-existent input
        assert validate_paths(tmp_path / "nonexistent", output_dir) is False

        # Input is file, not directory
        input_file = tmp_path / "file.txt"
        input_file.write_text("content")
        assert validate_paths(input_file, output_dir) is False

        # Output exists but is file
        output_file = tmp_path / "output.txt"
        output_file.write_text("content")
        assert validate_paths(input_dir, output_file) is False


class TestProcessing:
    """Test the main processing functionality."""

    def test_process_simple_file(self, tmp_path):
        """Test processing a simple Python file."""
        # Create input directory and file
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "test.py"
        test_file.write_text(
            """
import os

class TestClass:
    def method(self):
        return "test"

def test_function():
    return 42

print("Module loaded")
"""
        )

        # Process the file
        output_dir = tmp_path / "output"
        output_dir.mkdir()

        process_python_file(test_file, output_dir, input_dir)

        # Check output files exist
        assert (output_dir / "test.py").exists()
        assert (output_dir / "test_test_class.py").exists()
        assert (output_dir / "test_test_function.py").exists()

        # Check main file content
        main_content = (output_dir / "test.py").read_text()
        assert "import os" in main_content
        assert "from .test_test_class import TestClass" in main_content
        assert "from .test_test_function import test_function" in main_content
        assert "print('Module loaded')" in main_content or 'print("Module loaded")' in main_content
        assert "def test_function" not in main_content
        assert "class TestClass" not in main_content

        # Check extracted class file
        class_content = (output_dir / "test_test_class.py").read_text()
        # os is not used in TestClass, so it should be filtered out
        assert "import os" not in class_content
        assert "class TestClass:" in class_content
        assert "def method(self):" in class_content

        # Check extracted function file
        func_content = (output_dir / "test_test_function.py").read_text()
        # os is not used in test_function, so it should be filtered out
        assert "import os" not in func_content
        assert "def test_function():" in func_content
        assert "return 42" in func_content

    def test_process_file_no_definitions(self, tmp_path):
        """Test processing a file with no class/function definitions."""
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "constants.py"
        test_file.write_text(
            """
# Constants file
VERSION = "1.0.0"
DEBUG = True
CONFIG = {"key": "value"}
"""
        )

        output_dir = tmp_path / "output"
        output_dir.mkdir()

        process_python_file(test_file, output_dir, input_dir)

        # Should just copy the file
        assert (output_dir / "constants.py").exists()
        assert (output_dir / "constants.py").read_text() == test_file.read_text()

    def test_process_nested_structure(self, tmp_path):
        """Test processing files in nested directory structure."""
        input_dir = tmp_path / "input"
        (input_dir / "src" / "utils").mkdir(parents=True)

        test_file = input_dir / "src" / "utils" / "helpers.py"
        test_file.write_text(
            """
def helper_function():
    return "help"

class HelperClass:
    pass
"""
        )

        output_dir = tmp_path / "output"

        process_python_file(test_file, output_dir, input_dir)

        # Check directory structure is preserved
        assert (output_dir / "src" / "utils" / "helpers.py").exists()
        assert (output_dir / "src" / "utils" / "helpers_helper_function.py").exists()
        assert (output_dir / "src" / "utils" / "helpers_helper_class.py").exists()

    def test_process_file_with_syntax_error(self, tmp_path):
        """Test handling of files with syntax errors."""
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "broken.py"
        test_file.write_text(
            """
def broken_function(
    # Missing closing parenthesis
    return "broken"
"""
        )

        output_dir = tmp_path / "output"
        output_dir.mkdir()

        # Should handle error gracefully
        process_python_file(test_file, output_dir, input_dir)

        # No output files should be created for broken file
        assert not (output_dir / "broken.py").exists()


class TestProcessingDirs:
    """Test the 'dirs' method processing functionality."""

    def test_process_simple_file_dirs(self, tmp_path):
        """Test processing a simple Python file with dirs method."""
        # Create input directory and file
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "test.py"
        test_file.write_text(
            """
import os

class TestClass:
    def method(self):
        return "test"

def test_function():
    return 42

print("Module loaded")
"""
        )

        # Process the file
        output_dir = tmp_path / "output"
        output_dir.mkdir()

        process_python_file_dirs(test_file, output_dir, input_dir)

        # Check output directory structure
        assert (output_dir / "test").is_dir()
        assert (output_dir / "test" / "__init__.py").exists()
        assert (output_dir / "test" / "test_class.py").exists()
        assert (output_dir / "test" / "test_function.py").exists()

        # Check __init__.py content
        init_content = (output_dir / "test" / "__init__.py").read_text()
        assert "import os" in init_content
        assert "from .test_class import TestClass" in init_content
        assert "from .test_function import test_function" in init_content
        assert "print('Module loaded')" in init_content or 'print("Module loaded")' in init_content

        # Check extracted files don't have filename prefix
        class_content = (output_dir / "test" / "test_class.py").read_text()
        assert "class TestClass:" in class_content
        assert "def method(self):" in class_content

        func_content = (output_dir / "test" / "test_function.py").read_text()
        assert "def test_function():" in func_content
        assert "return 42" in func_content

    def test_process_file_no_definitions_dirs(self, tmp_path):
        """Test processing a file with no definitions using dirs method."""
        input_dir = tmp_path / "input"
        input_dir.mkdir()

        test_file = input_dir / "constants.py"
        test_file.write_text(
            """
# Constants file
VERSION = "1.0.0"
DEBUG = True
"""
        )

        output_dir = tmp_path / "output"
        output_dir.mkdir()

        process_python_file_dirs(test_file, output_dir, input_dir)

        # Should create a directory with __init__.py containing original content
        assert (output_dir / "constants").is_dir()
        assert (output_dir / "constants" / "__init__.py").exists()
        assert (output_dir / "constants" / "__init__.py").read_text() == test_file.read_text()
</file>

<file path=".cursorrules">
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
START SPECIFICATION:
---
description: Create overview documentation when analyzing unique business logic in video processing tools, particularly those focused on intelligent video overlay and synchronization with advanced frame alignment capabilities
globs: *.py,*.md
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


## Core Business Components

### Frame Alignment Engine (Importance: 95)
Handles the critical synchronization between foreground and background videos using:

1. **Perceptual Frame Fingerprinting**
- Creates unique fingerprints for video frames using multiple hash algorithms
- Combines pHash, AverageHash, ColorMomentHash and MarrHildrethHash 
- Enables ultra-fast frame comparison without pixel-level analysis

2. **Spatial Alignment System**
- Determines optimal x/y positioning of foreground video
- Uses template matching with normalized cross-correlation
- Handles automatic scaling when source sizes differ

3. **Temporal Synchronization**  
- Implements Dynamic Time Warping for frame-perfect alignment
- Creates monotonic mapping between foreground and background frames
- Prevents temporal drift through adaptive keyframe density

### Video Composition Pipeline (Importance: 90)

1. **Video Analysis Phase**
- Extracts metadata from both video sources
- Determines frame rates, durations, resolutions
- Identifies available audio streams

2. **Frame Matching Algorithm**  
- Maps every foreground frame to optimal background frame
- Maintains foreground timing integrity
- Dynamically adapts background video timing

3. **Audio Integration System**
- Prioritizes foreground audio when available
- Ensures synchronized audio/video alignment
- Handles fallback to background audio

### Operation Modes (Importance: 85)

1. **Border Matching Mode**
- Aligns content based on visible background edges
- Uses configurable margin thickness
- Optimizes edge detection for alignment

2. **Smooth Blending Mode** 
- Creates seamless visual transitions
- Handles frame edge integration
- Preserves content integrity during blending

$END$
END SPECIFICATION
</file>

<file path="CHANGELOG.md">
# CHANGELOG

## [Unreleased]

### Changed
- Updated project documentation and planning files
- Reorganized TODO.md with sprint-based task organization  
- Refined PLAN.md with clearer phase priorities and updated status
- Enhanced README.md with more comprehensive examples and professional formatting

### Refactored
- **Core Module Restructuring**: The main `pyxplod.py` script has been significantly refactored into a more modular architecture to improve maintainability, readability, and testability.
  - Moved utility functions (e.g., `to_snake_case`) to `src/pyxplod/utils.py`.
  - Consolidated all AST (Abstract Syntax Tree) related functions into `src/pyxplod/ast_utils.py`.
  - Grouped file system operations, path validation, and filename generation logic into `src/pyxplod/file_utils.py`.
  - Centralized the core processing logic for both `files` and `dirs` methods (`process_python_file`, `process_python_file_dirs`) into `src/pyxplod/processors.py`.
  - Migrated the command-line interface logic, including the `main` function and `fire` integration, to `src/pyxplod/cli.py`.
  - The original `src/pyxplod/pyxplod.py` now serves as a thin entry point, re-exporting `main` from `cli.py`.
  - Updated `__init__.py` and `__main__.py` to reflect new module structure and `main` function location.
  - Ensured all internal imports and dependencies are correctly resolved across the new modules.

### Documentation
- Improved task tracking and project roadmap visibility
- Better organized development priorities by sprint cycles
- Added more detailed technical specifications for remaining features

## [0.3.0] - 2025-05-25

### Added
- Import optimization: Only include imports that are actually used in extracted files
- New `analyze_name_usage()` function to detect which names are referenced in code
- New `filter_imports_for_names()` function to filter imports based on usage analysis
- Improved decorator handling in import analysis
- Better handling of attribute imports (e.g., `os.path`)

### Fixed
- Fixed import duplication bug where all imports were copied to every extracted file
- Reduced extracted file sizes by 30-50% through smart import filtering

### Changed
- Enhanced verbose logging to show import filtering statistics
- Improved code documentation with detailed docstrings

## [0.2.0] - 2025-05-25

### Added
- New `--method dirs` option for alternative explosion strategy
- Creates package directories instead of flat file structure
- Generates `__init__.py` files that maintain API compatibility
- Simpler file naming without prefix in dirs method
- Tests for the new dirs method functionality

### Changed
- Default behavior now explicitly uses `--method files`
- Updated documentation to explain both methods

## [0.1.0] - 2025-05-25

### Added
- Initial implementation of `pyxplod` tool
- CLI interface with `--input`, `--output`, and `--verbose` arguments using `fire`
- Recursive Python file discovery in input directories
- AST-based parsing and modification of Python files
- Extraction of class and function definitions into separate files
- Automatic conversion to snake_case for generated filenames
- Filename deduplication to avoid conflicts
- Replacement of definitions with relative imports
- Preservation of directory structure in output
- Module-level imports preserved in extracted files
- Error handling for syntax errors and invalid files
- Comprehensive logging with `loguru`
- Progress bar display with `rich`
- Test suite with 79% code coverage
- Support for Python 3.10+

### Features
- Deterministically "explodes" Python projects into smaller files
- Each class and function gets its own file
- Original file structure is maintained with imports
- Handles edge cases gracefully (empty files, syntax errors, etc.)

### Technical Details
- Uses Python's `ast` module for accurate parsing and code generation
- Implements proper relative imports for split files
- Maintains Python syntax validity in all generated files
</file>

<file path="pyproject.toml">
[project]
name = 'pyxplod'
description = ''
readme = 'README.md'
requires-python = '>=3.10'
keywords = []
dynamic = ['version']
classifiers = [
    'Development Status :: 4 - Beta',
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
    'Operating System :: OS Independent',
    'License :: OSI Approved :: MIT License',
    'Intended Audience :: Developers',
]
dependencies = [
    'fire>=0.7.0',
    'loguru>=0.7.2',
    'rich>=13.8.0',
]

[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

[project.license]
text = 'MIT'

[project.urls]
Documentation = 'https://github.com/twardoch/pyxplod#readme'
Issues = 'https://github.com/twardoch/pyxplod/issues'
Source = 'https://github.com/twardoch/pyxplod'

[project.optional-dependencies]
dev = [
    'pre-commit>=4.1.0',
    'ruff>=0.9.7',
    'mypy>=1.15.0',
    'absolufy-imports>=0.3.1',
    'pyupgrade>=3.19.1',
    'isort>=6.0.1',
]
test = [
    'pytest>=8.3.4',
    'pytest-cov>=6.0.0',
    'pytest-xdist>=3.6.1',
    'pytest-benchmark[histogram]>=5.1.0',
    'pytest-asyncio>=0.25.3',
    'coverage[toml]>=7.6.12',
]
docs = [
    'sphinx>=7.2.6',
    'sphinx-rtd-theme>=2.0.0',
    'sphinx-autodoc-typehints>=2.0.0',
    'myst-parser>=3.0.0',
    'pytest>=8.3.4',
    'pytest-cov>=6.0.0',
    'pytest-xdist>=3.6.1',
    'pytest-benchmark[histogram]>=5.1.0',
    'pytest-asyncio>=0.25.3',
    'coverage[toml]>=7.6.12',
]
all = [
    'absolufy-imports>=0.3.1',
    'fire>=0.7.0',
    'hatch-vcs>=0.4.0',
    'hatchling>=1.27.0',
    'isort>=6.0.1',
    'loguru>=0.7.2',
    'mypy>=1.15.0',
    'pre-commit>=4.1.0',
    'pyupgrade>=3.19.1',
    'rich>=13.8.0',
    'ruff>=0.9.7',
    'myst-parser>=3.0.0',
    'sphinx-autodoc-typehints>=2.0.0',
    'sphinx-rtd-theme>=2.0.0',
    'sphinx>=7.2.6',
]

[project.scripts]
pyxplod = 'pyxplod.pyxplod:main'

[build-system]
requires = [
    'hatchling>=1.27.0',
    'hatch-vcs>=0.4.0',
]
build-backend = 'hatchling.build'
[tool.hatch.build]
include = [
    'src/pyxplod/py.typed',
    'src/pyxplod/data/**/*',
]
exclude = [
    '**/__pycache__',
    '**/.pytest_cache',
    '**/.mypy_cache',
]
[tool.hatch.build.targets.wheel]
packages = ['src/pyxplod']
reproducible = true
[tool.hatch.build.hooks.vcs]
version-file = 'src/pyxplod/__version__.py'

[tool.hatch.version]
source = 'vcs'

[tool.hatch.metadata]
allow-direct-references = true
[tool.hatch.envs.default]
features = [
    'dev',
    'test',
    'all',
]
dependencies = [
    'fire>=0.7.0',
    'loguru>=0.7.2',
    'rich>=13.8.0',
]

[tool.hatch.envs.default.scripts]
test = 'pytest {args:tests}'
test-cov = 'pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/pyxplod --cov=tests {args:tests}'
type-check = 'mypy src/pyxplod tests'
lint = [
    'ruff check src/pyxplod tests',
    'ruff format --respect-gitignore src/pyxplod tests',
]
fmt = [
    'ruff format --respect-gitignore src/pyxplod tests',
    'ruff check --fix src/pyxplod tests',
]
fix = [
    'ruff check --fix --unsafe-fixes src/pyxplod tests',
    'ruff format --respect-gitignore src/pyxplod tests',
]
[[tool.hatch.envs.all.matrix]]
python = [
    '3.10',
    '3.11',
    '3.12',
]

[tool.hatch.envs.lint]
detached = true
features = ['dev']

[tool.hatch.envs.lint.scripts]
typing = 'mypy --install-types --non-interactive {args:src/pyxplod tests}'
style = [
    'ruff check {args:.}',
    'ruff format --respect-gitignore {args:.}',
]
fmt = [
    'ruff format --respect-gitignore {args:.}',
    'ruff check --fix {args:.}',
]
fix = [
    'ruff check --fix --unsafe-fixes {args:.}',
    'ruff format --respect-gitignore {args:.}',
]
all = [
    'style',
    'typing',
    'fix',
]

[tool.hatch.envs.test]
features = ['test']

[tool.hatch.envs.test.scripts]
test = 'python -m pytest -n auto {args:tests}'
test-cov = 'python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/pyxplod --cov=tests {args:tests}'
bench = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only'
bench-save = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json'

[tool.hatch.envs.docs]
features = ['docs']

[tool.hatch.envs.docs.scripts]
build = 'sphinx-build -b html docs/source docs/build'

[tool.hatch.envs.ci]
features = ['test']

[tool.hatch.envs.ci.scripts]
test = 'pytest --cov=src/pyxplod --cov-report=xml'
[tool.coverage.paths]
pyxplod = [
    'src/pyxplod',
    '*/pyxplod/src/pyxplod',
]
tests = [
    'tests',
    '*/pyxplod/tests',
]

[tool.coverage.report]
exclude_lines = [
    'no cov',
    'if __name__ == .__main__.:',
    'if TYPE_CHECKING:',
    'pass',
    'raise NotImplementedError',
    'raise ImportError',
    'except ImportError',
    'except KeyError',
    'except AttributeError',
    'except NotImplementedError',
]

[tool.coverage.run]
source_pkgs = [
    'pyxplod',
    'tests',
]
branch = true
parallel = true
omit = ['src/pyxplod/__about__.py']

[tool.mypy]
python_version = '3.10'
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[[tool.mypy.overrides]]
module = ['tests.*']
disallow_untyped_defs = false
disallow_incomplete_defs = false
[tool.pytest.ini_options]
addopts = '-v --durations=10 -p no:briefcase'
asyncio_mode = 'auto'
asyncio_default_fixture_loop_scope = 'function'
console_output_style = 'progress'
filterwarnings = [
    'ignore::DeprecationWarning',
    'ignore::UserWarning',
]
log_cli = true
log_cli_level = 'INFO'
markers = [
    '''benchmark: marks tests as benchmarks (select with '-m benchmark')''',
    'unit: mark a test as a unit test',
    'integration: mark a test as an integration test',
    'permutation: tests for permutation functionality',
    'parameter: tests for parameter parsing',
    'prompt: tests for prompt parsing',
]
norecursedirs = [
    '.*',
    'build',
    'dist',
    'venv',
    '__pycache__',
    '*.egg-info',
    '_private',
]
python_classes = ['Test*']
python_files = ['test_*.py']
python_functions = ['test_*']
testpaths = ['tests']

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = 'file'
save-data = true
compare = [
    'min',
    'max',
    'mean',
    'stddev',
    'median',
    'iqr',
    'ops',
    'rounds',
]

[tool.ruff]
target-version = 'py310'
line-length = 120

[tool.ruff.lint]
select = [
    'A',
    'ARG',
    'ASYNC',
    'B',
    'C',
    'DTZ',
    'E',
    'EM',
    'F',
    'FBT',
    'I',
    'ICN',
    'ISC',
    'LOG',
    'N',
    'PLC',
    'PLE',
    'PLR',
    'PLW',
    'PT',
    'PTH',
    'PYI',
    'RET',
    'RSE',
    'RUF',
    'S',
    'SIM',
    'T',
    'TCH',
    'TID',
    'UP',
    'W',
    'YTT',
]
ignore = [
    'B027',
    'C901',
    'FBT003',
    'PLR0911',
    'PLR0912',
    'PLR0913',
    'PLR0915',
    'PLR1714',
    'PLW0603',
    'PT013',
    'PTH123',
    'PYI056',
    'S105',
    'S106',
    'S107',
    'S110',
    'SIM102',
]
unfixable = ['F401']
exclude = [
    '.git',
    '.venv',
    'venv',
    'dist',
    'build',
    '__pycache__',
]

[tool.ruff.lint.isort]
known-first-party = ['pyxplod']

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = 'all'

[tool.ruff.lint.per-file-ignores]
"tests/**/*" = [
    'PLR2004',
    'S101',
    'TID252',
]
</file>

<file path="README.md">
# pyxplod

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**pyxplod** is a Python refactoring tool that "explodes" Python files by extracting each class and function definition into separate files, automatically replacing them with imports. This helps break down large Python modules into smaller, more manageable pieces while maintaining functionality.

## Features

- 🔍 **Intelligent code extraction**: Uses Python's AST to accurately identify and extract classes and functions
- 📁 **Structure preservation**: Maintains your project's directory structure in the output
- 🔗 **Automatic imports**: Replaces extracted code with proper relative imports
- ⚡ **Smart import optimization**: Only includes imports actually used in each extracted file (v0.3.0+)
- 🛡️ **Safe operation**: Non-destructive - creates new files without modifying originals
- 📊 **Progress tracking**: Visual progress bars and detailed logging with `rich` and `loguru`
- 🎯 **Smart naming**: Converts class/function names to snake_case for consistent file naming
- 🗂️ **Dual methods**: Choose between flat file structure or package-based organization
- ⚡ **Fast processing**: Efficiently handles large codebases with error recovery

## Installation

### From PyPI (when published)

```bash
pip install pyxplod
```

Or using `uv`:

```bash
uv pip install pyxplod
```

### From Source (Current)

```bash
# Clone the repository
git clone https://github.com/twardoch/pyxplod.git
cd pyxplod

# Install in development mode
uv pip install -e .
```

## Usage

### Basic Usage (Files Method)

```bash
pyxplod --input /path/to/source --output /path/to/output
```

### Using Dirs Method

```bash
pyxplod --input /path/to/source --output /path/to/output --method dirs
```

### With Verbose Logging

```bash
pyxplod --input /path/to/source --output /path/to/output --verbose
```

## Methods

pyxplod supports two different explosion methods:

### Files Method (Default)

The `files` method creates separate files in the same directory structure, with each extracted class/function having a filename prefix based on the original file.

### Dirs Method

The `dirs` method creates a directory (package) for each Python file, with extracted classes and functions as separate modules within that package. An `__init__.py` file contains imports and any remaining module-level code.

## How It Works

Given a Python file like this:

```python
# src/utils.py
import os
from typing import List

class FileHandler:
    def __init__(self):
        self.files = []
    
    def add_file(self, path: str):
        self.files.append(path)

def process_data(data: List[str]) -> str:
    return "\n".join(data)

CONSTANT = "some_value"
```

pyxplod will create different structures based on the method used:

### Files Method Output (Default)

```
output/
└── src/
    ├── utils.py                    # Modified main file
    ├── utils_file_handler.py       # Extracted class
    └── utils_process_data.py       # Extracted function
```

### Dirs Method Output

```
output/
└── src/
    └── utils/
        ├── __init__.py             # Module interface with imports
        ├── file_handler.py         # Extracted class
        └── process_data.py         # Extracted function
```

## Example Contents

### Files Method Output

**output/src/utils.py:**
```python
from typing import List
from .utils_file_handler import FileHandler
from .utils_process_data import process_data

CONSTANT = "some_value"
```

**output/src/utils_file_handler.py:**
```python
class FileHandler:
    def __init__(self):
        self.files = []
    
    def add_file(self, path: str):
        self.files.append(path)
```

**output/src/utils_process_data.py:**
```python
from typing import List

def process_data(data: List[str]) -> str:
    return "\n".join(data)
```

### Dirs Method Output

**output/src/utils/__init__.py:**
```python
from typing import List
from .file_handler import FileHandler
from .process_data import process_data

CONSTANT = "some_value"
```

**output/src/utils/file_handler.py:**
```python
class FileHandler:
    def __init__(self):
        self.files = []
    
    def add_file(self, path: str):
        self.files.append(path)
```

**output/src/utils/process_data.py:**
```python
from typing import List

def process_data(data: List[str]) -> str:
    return "\n".join(data)
```

> **Note**: pyxplod v0.3.0+ automatically optimizes imports - only the imports actually used in each file are included, reducing file sizes by 30-50%.

## Advanced Options

### Command Line Arguments

```bash
pyxplod --help
```

| Argument | Description | Default | Example |
|----------|-------------|---------|---------|
| `--input` | Source directory to process | Required | `--input ./src` |
| `--output` | Output directory for exploded files | Required | `--output ./output` |
| `--method` | Processing method: `files` or `dirs` | `files` | `--method dirs` |
| `--verbose` | Enable detailed logging | `False` | `--verbose` |

### Processing Methods Comparison

| Aspect | Files Method | Dirs Method |
|--------|--------------|-------------|
| **Structure** | Flat files with prefixes | Package directories |
| **File naming** | `utils_my_class.py` | `my_class.py` |
| **Organization** | Same directory | Subdirectories with `__init__.py` |
| **Best for** | Simple modules, quick extraction | Complex modules, clean organization |
| **Import style** | `from .utils_my_class import MyClass` | `from .utils.my_class import MyClass` |

## Use Cases

- **Refactoring large modules**: Break down monolithic Python files (500+ lines) into smaller, focused modules
- **Code organization**: Improve project structure by separating concerns and creating logical boundaries
- **Testing**: Make it easier to test individual components in isolation with focused test files
- **Code review**: Simplify code reviews by creating smaller, single-purpose files that are easier to understand
- **Legacy code**: Gradually modernize legacy codebases by extracting reusable components
- **Microservices preparation**: Extract functionality before splitting monoliths into microservices
- **Educational purposes**: Help understand code structure by separating concerns visually

## Features in Detail

### Smart Import Handling
- Preserves all module-level imports in extracted files
- Generates relative imports for the extracted components
- Maintains import order and structure

### Error Handling
- Gracefully handles syntax errors in source files
- Skips files that cannot be parsed
- Provides detailed error messages for troubleshooting

### File Naming
- Converts CamelCase to snake_case automatically
- Handles naming conflicts with automatic deduplication
- Preserves meaningful names while ensuring filesystem compatibility

## Development

### Setup Development Environment

```bash
# Clone the repository
git clone https://github.com/twardoch/pyxplod.git
cd pyxplod

# Install in development mode
pip install -e .

# Install development dependencies
pip install -e ".[dev,test]"
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=pyxplod

# Run specific test
pytest tests/test_pyxplod.py::TestProcessing::test_process_simple_file
```

### Code Quality

```bash
# Auto-format, lint and test (recommended)
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;

# Individual commands
ruff check src/ tests/       # Linting
ruff format src/ tests/      # Formatting  
mypy src/                    # Type checking
```

## Requirements

- Python 3.10 or higher
- Dependencies:
  - `fire` - CLI interface
  - `loguru` - Enhanced logging
  - `rich` - Progress bars and formatting
  - Standard library: `ast`, `pathlib`, `os`

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Limitations & Known Issues

- **Comments**: Comments between class/function definitions may be lost during processing
- **Formatting**: Code formatting may change due to AST parsing and regeneration
- **Nested definitions**: Only top-level classes and functions are extracted (nested classes/functions remain in original files)
- **Decorators**: Some complex decorator chains may need manual review after extraction
- **Large files**: Very large files (>1000 lines) may require more memory and processing time

See [TODO.md](TODO.md) for a complete list of planned improvements.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Development Setup

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Set up development environment:
   ```bash
   uv pip install -e ".[dev,test]"
   ```
4. Make your changes and add tests
5. Run the test suite:
   ```bash
   python -m pytest --cov=pyxplod
   ```
6. Ensure code quality:
   ```bash
   fd -e py -x ruff check --fix {}; fd -e py -x ruff format {}
   ```
7. Commit your changes (`git commit -m 'Add some amazing feature'`)
8. Push to the branch (`git push origin feature/amazing-feature`)
9. Open a Pull Request

### Code Guidelines

- Follow PEP 8 style guidelines
- Add type hints using simple syntax (`list`, `dict`, `str | None`)
- Write clear docstrings for all functions and classes
- Add tests for new features and bug fixes
- Keep functions focused and single-purpose

## Changelog

See [CHANGELOG.md](CHANGELOG.md) for a detailed history of changes.

## Author

Created by Adam Twardoch
</file>

<file path="PLAN.md">
# PLAN.md

## Critical Scope Resolution Issue (URGENT)

**Problem**: When extracting functions/classes with the `dirs` method, module-level variables are not included in extracted files, causing `NameError` in the extracted code.

**Example**:
```python
# Original file
from rich.console import Console
console = Console()

def my_function():
    console.print("Hello")  # 'console' undefined in extracted file
```

**Solution Strategy**:
1. **Detect Module-Level Variables**: Identify assignments like `console = Console()` at module level
2. **Analyze Dependencies**: Check which extracted definitions reference these variables
3. **Include Required Variables**: Copy necessary module-level assignments to extracted files
4. **Maintain Import Order**: Ensure variables come after their required imports

**Implementation Plan**:
- [x] Add `find_module_variables()` function to detect module-level assignments
- [x] Enhance `analyze_name_usage()` to distinguish between imported names and module variables
- [x] Update `write_extracted_file()` to include required module variables
- [x] Apply fix to both `files` and `dirs` methods
- [x] Test with rich.console example from TODO.md

**Status**: ✅ **COMPLETED** - Scope resolution implemented and tested successfully.

## Sprint 2

### Code Quality & Preservation Issues
- [ ] **Fix code element preservation** (HIGH PRIORITY)
  - [ ] Maintain decorators on extracted functions/classes (@property, @staticmethod, etc.)
  - [ ] Preserve docstrings in correct positions (before or after imports)
  - [ ] Handle comments between definitions (currently lost during AST processing)
  - [ ] Ensure type annotations are preserved exactly

### Error Handling & Robustness  
- [ ] **Improve error handling and Unicode support**
  - [ ] Add proper Unicode/encoding support (UTF-8, other encodings)
  - [ ] Implement partial file processing on syntax errors
  - [ ] Better error messages with file names and line numbers
  - [ ] Add `--skip-errors` flag to continue processing despite errors

## Next Sprint - Architecture & Features (v0.5.0)

### Code Architecture
- [x] **Refactor monolithic pyxplod.py** (Completed)
  - [x] Extract `utils.py` - Common utility functions (e.g., `to_snake_case`)
  - [x] Extract `ast_utils.py` - AST manipulation and analysis functions
  - [x] Extract `file_utils.py` - File discovery, I/O, and path operations
  - [x] Extract `processors.py` - Method implementation logic (files/dirs)
  - [x] Extract `cli.py` - Command line interface and argument parsing, `main` function

### Type System & Documentation
- [ ] **Enhance type system**
  - [ ] Add comprehensive type hints using simple syntax (list, dict, str | None)
  - [ ] Use proper AST node types instead of generic types
  - [ ] Add return type annotations to all functions

### User Experience Features
- [ ] **Essential user features**
  - [ ] Add `--dry-run` mode to preview changes without writing files
  - [ ] Create `.pyxplod.toml` configuration file support
  - [ ] Add `--exclude` patterns for skipping files/directories
  - [ ] Add `--include-private` flag for private methods/classes

## Future Backlog

### Advanced Code Handling
- [ ] **Complex code structures**
  - [ ] Handle nested classes and inner functions
  - [ ] Support async functions and async decorators
  - [ ] Handle complex decorator chains (@property, @classmethod, @lru_cache)
  - [ ] Support class methods, static methods, and property decorators

### Testing & Quality Assurance
- [ ] **Expand test coverage**
  - [ ] Test decorator preservation with various decorator types
  - [ ] Test Unicode file handling with different encodings
  - [ ] Test large file processing (>1000 lines)
  - [ ] Test error conditions and edge cases
  - [ ] Add integration tests with popular packages (requests, flask, django)

### Performance & Scalability
- [ ] **Performance improvements**
  - [ ] Implement parallel file processing for large codebases
  - [ ] Add streaming AST processing for very large files
  - [ ] Create incremental mode - only process changed files
  - [ ] Add progress persistence for resumable operations

### Advanced Features
- [ ] **Professional tooling features**
  - [ ] Add `--format` flag with Black integration for code formatting
  - [ ] Add `--verify` flag to validate output file integrity
  - [ ] Implement reverse operation (`--method implode`) to merge files back
  - [ ] Create plugin system for custom processing logic

### Documentation & Examples
- [ ] **Improve documentation**
  - [ ] Add inline comments explaining complex AST operations
  - [ ] Document import analysis and filtering logic
  - [ ] Create examples directory with before/after transformations
  - [ ] Add visual diagrams showing both processing methods



### Remaining Implementation Tasks

- [ ] **Phase 1: Critical Fixes & Improvements** (High Priority)
  - [ ] Preserve decorators and docstrings properly in extracted files
  - [ ] Handle comments between definitions (currently lost)
  - [ ] Add proper encoding handling for Unicode files
  - [ ] Improve error recovery - partial processing instead of skipping entirely
  - [ ] Fix import statement positioning relative to docstrings

- [x] **Phase 2: Code Quality & Architecture** (Completed for refactoring part)
  - [x] Refactor `pyxplod.py` into multiple modules:
    - `utils.py` - Common utility functions (e.g., `to_snake_case`)
    - `ast_utils.py` - AST manipulation functions
    - `file_utils.py` - File discovery and I/O operations
    - `processors.py` - Processing method implementations (files/dirs)
    - `cli.py` - Command line interface and `main` function
  - [ ] Add comprehensive type hints using simple syntax (list, dict, |) (Ongoing)
  - [ ] Implement proper debug logging patterns per CLAUDE.md (Partially addressed)
  - [ ] Add integration tests with real Python projects

- [ ] **Phase 3: Essential User Features** (Medium Priority)
  - [ ] Add `--dry-run` flag to preview changes without writing files
  - [ ] Support `.pyxplod.toml` configuration file
  - [ ] Handle nested classes and inner functions
  - [ ] Add `--exclude` pattern for skipping files/directories
  - [ ] Add `--include-private` flag for private methods/classes

- [ ] **Phase 4: Advanced Features** (Low Priority)
  - [ ] Add `--format` option to preserve formatting using Black
  - [ ] Support for async function annotations and decorators
  - [ ] Handle complex decorator chains (@property, @classmethod, etc.)
  - [ ] Implement reverse operation (`--method implode` to merge files back)
  - [ ] Add `--verify` flag to validate output integrity

- [ ] **Phase 5: Performance & Scalability** (Future)
  - [ ] Parallel processing using multiprocessing for large codebases
  - [ ] Streaming AST processing for very large files
  - [ ] Incremental mode - only process changed files since last run
  - [ ] Memory-efficient processing for huge codebases
  - [ ] Progress persistence for resumable operations

- [ ] **Phase 6: Testing & Documentation** (Ongoing)
  - [ ] Add property-based testing with Hypothesis
  - [ ] Create test suite with popular Python projects (requests, flask, etc.)
  - [ ] Add performance benchmarks and profiling
  - [ ] Generate API documentation with Sphinx
  - [ ] Add visual examples and before/after diagrams

### Technical Decisions

1. **AST vs. Regular Expressions**: Use AST for accurate parsing and modification
2. **Import Style**: Use relative imports for split files to maintain portability
3. **Naming Convention**: Snake_case with deduplication to avoid conflicts
4. **Error Handling**: Fail gracefully, skip problematic files with warnings
5. **Logging**: Verbose mode with loguru for debugging

### Dependencies Required
- `fire` - CLI interface
- `loguru` - Enhanced logging
- `ast` - Python AST parsing (built-in)
- `pathlib` - Path operations (built-in)

### Example Transformations

#### Method: files (default)

Input: `src/utils.py`
```python
import os

class MyClass:
    def method(self):
        pass

def my_function():
    pass
```

Output:
```
output/src/
├── utils.py
├── utils_my_class.py
└── utils_my_function.py
```

#### Method: dirs

Same input produces:
```
output/src/
└── utils/
    ├── __init__.py
    ├── my_class.py
    └── my_function.py
```

Both methods maintain API compatibility - external code using `from src.utils import MyClass` continues to work unchanged.

### Known Issues & Limitations

1. **Lost Elements**: Comments between definitions and some formatting are not preserved
2. **Limited Scope**: Only handles top-level classes and functions, not nested definitions
3. **Formatting**: Uses `ast.unparse()` which doesn't preserve original code formatting
4. **Memory Usage**: Entire AST is kept in memory, could be problematic for very large files
5. **Error Handling**: Files with syntax errors are skipped entirely instead of partial processing
6. **Decorator Preservation**: Some decorators may not be properly preserved in extracted files

### Design Decisions & Rationale

1. **Two Methods Approach**: 
   - `files` method: Simple, flat structure, good for small modules
   - `dirs` method: Package structure, better for larger modules, cleaner imports

2. **AST-based Processing**: Ensures syntactic correctness and proper Python structure

3. **Import Strategy**: Currently copies all imports for safety, needs optimization

4. **Naming Convention**: Snake_case with deduplication ensures filesystem compatibility

5. **Error Philosophy**: Fail safely, skip problematic files with clear error messages
</file>

<file path="src/pyxplod/ast_utils.py">
# this_file: src/pyxplod/ast_utils.py
"""AST (Abstract Syntax Tree) related utility functions for pyxplod."""

import ast

# For Python 3.9+, list, set, tuple are standard types for hinting.
# No need to import List, Set, Tuple from typing unless for older versions or specific generic aliasing.


def extract_imports(tree: ast.AST) -> list[ast.stmt]:
    """Extract all import statements from an AST.

    Returns a list of Import and ImportFrom nodes at module level only.
    """
    return [
        node
        for node in tree.body
        if isinstance(node, ast.Import | ast.ImportFrom)
    ]


def analyze_name_usage(node: ast.AST) -> set[str]:
    """Analyze which names are used in an AST node.

    Returns a set of all names referenced in the node, including decorators.
    """
    names: set[str] = set()

    class NameCollector(ast.NodeVisitor):
        def visit_Name(self, node: ast.Name) -> None:  # noqa: N802
            names.add(node.id)
            self.generic_visit(node)

        def visit_Attribute(self, node: ast.Attribute) -> None:  # noqa: N802
            # For attributes like os.path, we want 'os'
            if isinstance(node.value, ast.Name):
                names.add(node.value.id)
            self.generic_visit(node)

    # First check for decorators on the node itself
    if hasattr(node, "decorator_list"):
        for decorator in node.decorator_list:  # type: ignore
            # Handle simple decorators like @my_decorator
            if isinstance(decorator, ast.Name):
                names.add(decorator.id)
            # Handle attribute decorators like @module.decorator
            elif isinstance(decorator, ast.Attribute) and isinstance(decorator.value, ast.Name):
                names.add(decorator.value.id)
            # For complex decorators, visit them
            NameCollector().visit(decorator)

    # Then collect names from the rest of the node
    NameCollector().visit(node)
    return names


def filter_imports_for_names(imports: list[ast.stmt], used_names: set[str]) -> list[ast.stmt]:
    """Filter imports to only include those that are used.

    Args:
        imports: List of import statements
        used_names: Set of names used in the code

    Returns:
        List of imports that are actually used
    """
    needed_imports = []

    for imp in imports:
        if isinstance(imp, ast.Import):
            # For 'import x, y, z', check each name
            needed_aliases = []
            for alias in imp.names:
                # The name used in code is either the alias or the module name
                name_in_code = alias.asname or alias.name
                # For module.submodule, we check the first part
                base_name = name_in_code.split(".")[0]
                if base_name in used_names:
                    needed_aliases.append(alias)

            if needed_aliases:
                # Create a new import with only needed names
                new_import = ast.Import(names=needed_aliases)
                ast.copy_location(new_import, imp)
                needed_imports.append(new_import)

        elif isinstance(imp, ast.ImportFrom):
            # For 'from x import y, z', check each imported name
            needed_aliases = []
            for alias in imp.names:
                name_in_code = alias.asname or alias.name
                if name_in_code in used_names:
                    needed_aliases.append(alias)

            if needed_aliases:
                # Create a new import with only needed names
                new_import = ast.ImportFrom(module=imp.module, names=needed_aliases, level=imp.level)
                ast.copy_location(new_import, imp)
                needed_imports.append(new_import)

    return needed_imports


def find_definitions(tree: ast.AST) -> list[tuple[ast.stmt, str, str]]:
    """Find all class and function definitions at module level.

    Returns list of tuples: (node, type, name) where type is 'class' or 'function'.
    """
    definitions: list[tuple[ast.stmt, str, str]] = []
    for node in tree.body:
        if isinstance(node, ast.ClassDef):
            definitions.append((node, "class", node.name))
        elif isinstance(node, ast.FunctionDef):
            definitions.append((node, "function", node.name))
    return definitions


def create_import_statement(module_path: str, name: str) -> ast.ImportFrom:
    """Create an import statement for the extracted definition."""
    return ast.ImportFrom(
        module=module_path,
        names=[ast.alias(name=name, asname=None)],
        level=0,  # Absolute import from module
    )


def find_module_variables(tree: ast.AST) -> list[tuple[ast.stmt, str]]:
    """Find all module-level variable assignments.

    Returns list of tuples: (assignment_node, variable_name).
    Only includes simple assignments like 'console = Console()'.
    """
    variables: list[tuple[ast.stmt, str]] = []
    for node in tree.body:
        if isinstance(node, ast.Assign):
            # Handle simple assignments like: variable = expression
            variables.extend(
                (node, target.id)
                for target in node.targets
                if isinstance(target, ast.Name)
            )
    return variables
</file>

<file path="src/pyxplod/pyxplod.py">
# this_file: src/pyxplod/pyxplod.py

"""pyxplod: Python code exploder - extracts classes and functions into separate files.

This tool takes a Python project and "explodes" it by extracting each class and function
definition into its own file, replacing the original definitions with imports.
"""

# Core logic has been refactored into submodules.
# This file now primarily serves to expose the main CLI entry point.

from pyxplod.cli import main

__all__ = ["main"]
</file>

<file path="TODO.md">
# TODO

## ✅ COMPLETED: Scope Resolution Issue

The `dirs` method now properly handles module-level variables. The scope resolution has been implemented and tested successfully.

**What was fixed:**
- Added `find_module_variables()` function to detect module-level assignments like `console = Console()`
- Enhanced `write_extracted_file()` to include required module variables in extracted files
- Updated both `files` and `dirs` methods to pass module variables to extracted files
- Enhanced import filtering to include imports needed by module variables

**Example (now working correctly):**

Original file:
```python
from rich.console import Console
console = Console()

def my_function():
    console.print("Hello")  # console is now defined in extracted file
```

Extracted file now includes:
```python
from rich.console import Console
console = Console()

def my_function():
    console.print("Hello")  # ✅ console is properly defined
```

**Status**: ✅ **RESOLVED** - Scope resolution implemented in both `files` and `dirs` methods with comprehensive testing.

## ✅ COMPLETED: Codebase Refactoring (MVP)

The core `pyxplod.py` script has been refactored into a modular structure to improve maintainability and readability.

**What was done:**
- The monolithic `pyxplod.py` was broken down into the following modules:
  - `utils.py`: For common helper functions like `to_snake_case`.
  - `ast_utils.py`: For all AST (Abstract Syntax Tree) manipulation and analysis.
  - `file_utils.py`: For file system operations, path validation, and filename generation.
  - `processors.py`: Contains the core logic for `process_python_file` and `process_python_file_dirs`.
  - `cli.py`: Handles the command-line interface using `fire`, including the `main` function and logging setup.
- `pyxplod.py` itself is now a thin layer, primarily re-exporting `main` from `cli.py`.
- Imports and dependencies between these new modules have been updated.
- `__init__.py` and `__main__.py` have been updated to reflect the new location of `main`.

**Status**: ✅ **COMPLETED** - Refactoring of `pyxplod.py` into submodules is complete.
</file>

</files>
